{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science](http://datascience-intro.github.io/1MS041-2022/)    \n",
    "## 1MS041, 2022 \n",
    "&copy;2022 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables\n",
    "\n",
    "A random variable is a mapping from the sample space $\\Omega$ to the set of real numbers $\\mathbb{R}$. In other words, it is a numerical value determined by the outcome of the experiment.\n",
    "\n",
    "We already saw *discrete random variables* that take values in a discrete set, of two types:\n",
    "\n",
    "- those with with finitely many values, eg. the two values in $\\{0,1\\}$ for the Bernoulli$(\\theta)$ RV \n",
    "- those with *countably infinitely many* values, eg. values in the set of all non-negative integers: $\\{0,1,2,\\ldots\\}$, for the 'infinite coin tossing experiment' that records the number of consecutive 'Tails' you see before the first 'Heads' occurs.\n",
    "\n",
    "Very often in data science there will be discrete random variables, often labels of things or indication of presence or absence of something, or simply just the number of things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use the computer to simulate a random variable, lets choose a fair coin with $0$ is tails and $1$ is heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "X = lambda : randint(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we observe `X` it will provide a new value, in this document that means we execute `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(X())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ is the random variable, it does not have a value, but it is a function. Each time we execute it we \"provide\" a new $\\omega$ and get a new observation $X(\\omega)$, but the computer \"hides\" the $\\omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution function\n",
    "\n",
    "According to the definition in the lecture notes given a real valued random variable $X: \\Omega \\to R$ we assign probability to the random variable as follows\n",
    "\n",
    "$$\n",
    "    F(x):=P(X \\leq x) = P(X^{[-1]}((-\\infty,x])) = P(\\{\\omega: X(\\omega) \\leq x\\})\n",
    "$$\n",
    "\n",
    "Let us consider the random variable $X$ from the spam/ham example and sms texts. In this case $X$ can only take two values, namely $0$ and $1$. In this case $X$ was a function from the text to a number, so the set\n",
    "\n",
    "$$\n",
    "    \\{\\omega \\in \\Omega: X(\\omega) \\leq 0\\} = \\{\\omega \\in \\Omega: X(\\omega) = 0\\} = \\{\\text{all non spam texts}\\}.\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "    \\{\\omega \\in \\Omega: X(\\omega) \\leq 1\\} = \\{\\omega \\in \\Omega: X(\\omega) = 1 \\text{ or } 0\\} = \\{\\text{all spam and non spam texts}\\}.\n",
    "$$\n",
    "\n",
    "#### YouTry\n",
    "\n",
    "What is $\\{\\omega \\in \\Omega: X(\\omega) \\leq -1\\}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at some distribution functions and what we can do with them.\n",
    "\n",
    "First up is the example above, lets plot the distribution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import plotEMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotEMF([(0,1/2),(1,1/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import plotEDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotEDF([(0,1/2),(1,1/2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $de~Moivre(\\frac{1}{k}, \\frac{1}{k}, \\ldots, \\frac{1}{k})$ RV\n",
    "\n",
    "We have seen that a $Bernoulli(\\theta)$ RV has two outcomes (0 and 1).  What if we are interested in modelling situations where there are more than two outcomes of interest? For example, we could use a $Bernoulli(\\frac{1}{2})$ RV to model whether the outcome of the flip of a fair coin is a head, but we can't use it for modelling a RV which is the number we get when we toss a six-sided die.  \n",
    "\n",
    "So, now, we will consider a natural generalization of the $Bernoulli(\\theta)$ RV with more than two outcomes. This is called the $de~Moivre(\\frac{1}{k}, \\frac{1}{k}, \\ldots, \\frac{1}{k})$ RV (after Abraham de Moivre, 1667-1754), one of the first great analytical probabalists).  \n",
    "\n",
    "A $de~Moivre(\\frac{1}{k}, \\frac{1}{k}, \\ldots, \\frac{1}{k})$ RV $X$ has a discrete uniform distribution over $\\{1, 2, ..., k\\}$:  there are $k$ possible equally probable or *equiprobable* values that the RV can take. \n",
    "\n",
    "If we are rolling a die that is a cube with six faces and $X$ is the number on the face of the die that touches the floor upon coming to a stop, then $k=6$. \n",
    "\n",
    "Or think of the New Zealand Lotto game.  There are 40 balls in the machine, numbered $1, 2, \\ldots, 40$.  The number on the first ball out of the machine can be modelled as a de Moivre$(\\frac{1}{40}, \\frac{1}{40}, \\ldots, \\frac{1}{40})$ RV. \n",
    "\n",
    "We say that an RV $X$ is de Moivre$(\\frac{1}{k}, \\frac{1}{k}, \\ldots, \\frac{1}{k})$ distributed if its probability mass function PMF is:\n",
    "\n",
    "$$\n",
    "f \\left(x; \\left( \\frac{1}{k}, \\frac{1}{k}, \\ldots, \\frac{1}{k} \\right) \\right) = \\begin{cases} 0 & \\quad \\text{if } x \\notin \\{1,2,\\ldots,k\\}\\\\ \\frac{1}{k} & \\quad \\text{if } x \\in \\{1,2,\\ldots,k\\} \\end{cases}\n",
    "$$\n",
    "\n",
    "We can find the expectation: \n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl} E(X) & = & \\sum_{x=1}^k xP(X=x)\\\\ &=& (1 \\times \\frac{1}{k}) + (2 \\times \\frac{1}{k}) + \\ldots + (k \\times \\frac{1}{k})\\\\ &=& (1 + 2 + \\dots + k)\\frac{1}{k}\\\\ &=& \\frac{k(k+1)}{2}\\frac{1}{k}\\\\ &=& \\frac{k+1}{2} \\, , \\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "the second moment: \n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl} E(X^2) & =& \\sum_{x=1}^k x^2P(X=x)\\\\ & =& (1^2 \\times \\frac{1}{k}) + (2^2 \\times \\frac{1}{k}) + \\ldots + (k^2 \\times \\frac{1}{k})\\\\ &=& (1^2 + 2^2 + \\dots + k^2)\\frac{1}{k}\\\\ &=& \\frac{k(k+1)(2k+1)}{6}\\frac{1}{k}\\\\ &=& \\frac{2k^2+3k+1}{6}\\, , \\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "and finally the variance: \n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl} V(X) &=& E(X^2) - \\left(E(X)\\right)^2\\\\ &=& \\frac{2k^2+3k+1}{6} - \\left( \\frac{k+1}{2} \\right)^2\\\\ &=&\\frac{2k^2+3k+1}{6} - \\frac{k^2 + 2k +1}{4}\\\\ &=& \\frac{4(2k^2 + 3k + 1) - 6(k^2 + 2k + 1) }{24}\\\\ &=& \\frac{8k^2 + 12k + 4 - 6k^2 - 12k - 6 }{24}\\\\ &=& \\frac{2k^2-2}{24} \\\\ &=& \\frac{k^2-1}{12} \\, . \\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "For a physical analog for a $k$-sided die consider the generalisation \n",
    "  - of a coin with two faces  \n",
    "  - to a toblerone bar with three sides, \n",
    "  - to a k-sided [regular polygon](https://en.wikipedia.org/wiki/Regular_polygon) *that is extended along the third dimension* so it can be rolled and we can define the outcome of the experiment to be the label of the face that touches the flat surface after coming to rest where the labels are in the set $\\{1,2,\\ldots,k\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simulate a deMoivre(1/40...) random variable, by again using `randint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "Y = lambda : randint(1,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(Y())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets see how the mass function and the distribution function looks like\n",
    "# for the deMoivre(1/40,...,1/40) i.e. 40 different outcomes, all with\n",
    "# the same probabilityb\n",
    "deMoivre = [(i,1/40) for i in range(1,41)]\n",
    "plotEMF(deMoivre)\n",
    "plotEDF(deMoivre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the code to produce the plots, check the file Utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from data\n",
    "\n",
    "Often we do not have access to the real distribution function but we have access to observations of the random variable. Lets simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have 1000 observations of the random variable Y above\n",
    "Y_obs = [Y() for i in range(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observations, we can estimate the probability of each value by counting how often they occured. This can be found in the makeEMF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import makeEMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotEMF(makeEMF(Y_obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot above, it really does not look like the theoretical one, but this is due to random chance. If we had more observations we would expect that the above is closer to theoretical. However, something which goes faster to the theoretical values are the distribution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotEDF(makeEMF(Y_obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This certainly looks closer to the theoretical distribution function. **It is a good idea to keep this in mind in the future, as this is a fundamental fact!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the example from the previous notebook, concerning the relative frequencies of the alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaspace = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q',\n",
    "              'R','S','T','U','V','W','X','Y','Z']\n",
    "alphaRelFreqs = [73/1000,9/1000,30/1000,44/1000,130/1000,28/1000,16/1000,35/1000,74/1000,\n",
    "                 2/1000,3/1000,35/1000, 25/1000,78/1000,74/1000,27/1000,3/1000,77/1000,63/1000,\n",
    "                 93/1000,27/1000,13/1000,16/1000,5/1000,19/1000,1/1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we identify 0->A, 1->B etc. we can plot it as\n",
    "plotEMF(list(enumerate(alphaRelFreqs)))\n",
    "plotEDF(list(enumerate(alphaRelFreqs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous random variable\n",
    "\n",
    "When a random variable takes on values in the continuum we call it a continuous RV.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "- Volume of water that fell on the Southern Alps yesterday (See video link below)\n",
    "- Vertical position above sea level, in micrometers, since the original release of a pollen grain at the head waters of a river\n",
    "- Resting position in degrees of a roulettet wheel after a brisk spin\n",
    "\n",
    "### Probability Density Function\n",
    "\n",
    "A RV $X$ with DF $F$ is called continuous if there exists a piece-wise continuous function $f$, called the  probability density function (PDF) $f$ of $X$, such that, for any $a$, $b \\in \\mathbb{R}$ with $a < b$,\n",
    "\n",
    "$$\n",
    "P(a < X \\le b) = F(b)-F(a) = \\int_a^b f(x) \\ dx \\ .\n",
    "$$\n",
    "\n",
    "\n",
    "The following hold for a continuous RV $X$ with PDF $f$:\n",
    "\n",
    "For any $x \\in \\mathbb{R}$, $P(X=x)=0$.\n",
    "Consequentially, for any $a,b \\in \\mathbb{R}$ with $a \\le b$ \n",
    "$$P(a < X < b ) = P(a < X \\le b) = P(a \\leq X \\le b) = P(a \\le X < b)$$\n",
    "By the fundamental theorem of calculus, except possibly at finitely many points (where the continuous pieces come together in the piecewise-continuous $f$): \n",
    "$$f(x) = \\frac{d}{dx} F(x)$$\n",
    "And of course $f$ must satisfy:\n",
    "$$\\int_{-\\infty}^{\\infty} f(x) \\ dx = P(-\\infty < X < \\infty) = 1$$\n",
    "\n",
    "\n",
    "#### You try at home\n",
    "Watch the Khan Academy [video about probability density functions](https://youtu.be/Fvi9A_tEmXQ) to warm-up to the meaning behind the maths above. Consider the continuous random variable $Y$ that measures the exact amount of rain tomorrow in inches. Think of the probability space $(\\Omega,\\mathcal{F},P)$ underpinning this random variable $Y:\\Omega \\to \\mathbb{Y}$. Here the sample space, range or support of the random variable $Y$ denoted by $\\mathbb{Y} = [0,\\infty) =\\{y : 0 \\leq y < \\infty\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showURL(url, ht=500):\n",
    "    \"\"\"Return an IFrame of the url to show in notebook with height ht\"\"\"\n",
    "    from IPython.display import IFrame\n",
    "    return IFrame(url, width='95%', height=ht) \n",
    "showURL('https://en.wikipedia.org/wiki/Integral',600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Uniform$(0,1)$ RV\n",
    "\n",
    "The Uniform$(0,1)$ RV is a continuous RV with a probability density function (PDF) that takes the value 1 if $x \\in [0,1]$ and $0$ otherwise.  Formally, this is written  \n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\mathbf{1}_{[0,1]}(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } 0 \\le x \\le 1 ,\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "and its distribution function (DF) or cumulative distribution function (CDF) is:\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "F(x) := \\int_{- \\infty}^x f(y) \\ dy =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } x < 0 , \\\\\n",
    "x & \\text{if } 0 \\le x \\leq 1 ,\\\\\n",
    "1 & \\text{if } x > 1\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Note that the DF is the identity map in $[0,1]$. \n",
    "\n",
    "The PDF, CDF and inverse CDF for a Uniform$(0,1)$ RV are shown below\n",
    "\n",
    "<img src=\"images/Uniform01ThreeCharts.png\" alt=\"Uniform01ThreeCharts\" width=500>\n",
    "\n",
    "The Uniform$(0,1)$ is sometimes called the Fundamental Model.\n",
    "\n",
    "The Uniform$(0,1)$ distribution comes from the Uniform$(a,b)$ family.   \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\mathbf{1}_{[a,b]}(x) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{(b-a)} & \\text{if } a \\le x \\le b,\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This is saying that if $X$ is a Uniform$(a,b)$ RV, then all values of $x$ between $a$ and $b$, i.e., $a \\le x \\le b$, are equally probable.   The Uniform$(0,1)$ RV is the member of the family where $a=0$, $b=1$.    \n",
    "\n",
    " The PDF and CDF for a Uniform$(a,b)$ RV are shown from wikipedia below\n",
    "\n",
    "<table style=\"width:95%\">\n",
    "  <tr>\n",
    "    <th><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Uniform_Distribution_PDF_SVG.svg/500px-Uniform_Distribution_PDF_SVG.svg.png\" alt=\"500px-Uniform_Distribution_PDF_SVG.svg.png\" width=250></th>\n",
    "    <th><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Uniform_cdf.svg/500px-Uniform_cdf.svg.png\" alt=\"wikipedia image 500px-Uniform_cdf.svg.png\" width=250></th> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "You can dive deeper into this family of random vaiables <a href=\"https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)\">here</a>.\n",
    "\n",
    "Python has a function for simulating samples from a Uniform$(a,b)$ distribution.  We will learn more about this later in the course. Let's go ahead and use it to simulate samples from it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform(-1,1)  # reevaluate the cell to see how the samples change upon each re-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectations\n",
    "\n",
    "The *expectation* of $X$ is also known as the *population mean*, *first moment*, or *expected value* of $X$.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "E\\left(X\\right) := \\int x \\, dF(x) =\n",
    "\\begin{cases}\n",
    "\\sum_x x \\, f(x) & \\qquad \\text{if }X \\text{ is discrete} \\\\\n",
    "\\int x \\, f(x)\\,dx  & \\qquad \\text{if } X \\text{ is continuous}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Sometimes, we denote $E(X)$ by $E X$ for brevity.  Thus, the expectation is a single-number summary of the RV $X$ and may be thought of  as the average.\n",
    "\n",
    "In general though, we can talk about the Expectation of a function $g$ of a RV $X$.  \n",
    "\n",
    "The Expectation of a function $g$ of a RV $X$ with DF $F$ is:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "E\\left(g(X)\\right) := \\int g(x)\\,dF(x) =\n",
    "\\begin{cases}\n",
    "\\sum_x g(x) f(x) & \\qquad \\text{if }X \\text{ is discrete} \\\\\n",
    "\\int g(x) f(x)\\,dx  & \\qquad \\text{if } X \\text{ is continuous}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "provided the sum or integral is well-defined.  We say the expectation exists if\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\int \\left|g(x)\\right|\\,dF(x) < \\infty \\ .\n",
    "\\end{equation}\n",
    "\n",
    "When we are looking at the Expectation of $X$ itself, we have $g(x) = x$\n",
    "\n",
    "Thinking about the Expectations like this, can you see that the familiar Variance of X is in fact the Expection of $g(x) = (x - E(X))^2$?\n",
    "\n",
    "The variance of $X$ (a.k.a. second moment)\n",
    "\n",
    "Let $X$ be a RV with mean or expectation $E(X)$.  The variance of $X$ denoted by $V(X)$ or $VX$ is\n",
    "\n",
    "$$\n",
    "V(X) := E\\left((X-E(X))^2\\right) = \\int (x-E(X))^2 \\,d F(x)\n",
    "$$\n",
    "\n",
    "provided this expectation exists.  The standard deviation denoted by $\\sigma(X) := \\sqrt{V(X)}$.\n",
    "\n",
    "Thus variance is a measure of ``spread'' of a distribution.\n",
    "\n",
    "The $k$-th moment of a RV comes from the Expectation of $g(x) = x^k$.\n",
    "\n",
    "We call\n",
    "\n",
    "$$\n",
    "E(X^k) = \\int x^k\\,dF(x)\n",
    "$$\n",
    "\n",
    "\n",
    "the $k$-th moment of the RV $X$ and say that the $k$-th moment exists when $E(|X|^k) < \\infty$.  \n",
    "\n",
    "\n",
    "#### Properties of Expectations\n",
    "\n",
    "\n",
    "\n",
    "1. If the $k$-th moment exists and if $j<k$ then the $j$-th moment exists.\n",
    "- If $X_1,X_2,\\ldots,X_n$ are RVs and $a_1,a_2,\\ldots,a_n$ are constants, then $E \\left( \\sum_{i=1}^n a_i X_i \\right) = \\sum_{i=1}^n a_i E(X_i)$\n",
    "- Let $X_1,X_2,\\ldots,X_n$ be independent RVs, then \n",
    "  - $E \\left(  \\prod_{i=1}^n X_i \\right) = \\prod_{i=1}^{n} E(X_i)$\n",
    "- $V(X) = E(X^2) - (E(X))^2$\n",
    "- If $a$ and $b$ are constants, then $V \\left(aX + b\\right) = a^2V(X)$\n",
    "- If $X_1,X_2,\\ldots,X_n$ are independent and $a_1,a_2,\\ldots,a_n$ are constants, then: $V \\left(  \\sum_{i=1}^n a_i X_i \\right) = \\sum_{i=1}^n a_i^2 V(X_i)$\n",
    "\n",
    "#### You try at home\n",
    "\n",
    "Watch the Khan Academy videos about [probability density functions](https://youtu.be/Fvi9A_tEmXQ) and [expected value](https://youtu.be/j__Kredt7vY) if you want to get another angle on the material more slowly step-by-step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The population mean and variance of the Bernoulli$(\\theta)$ RV\n",
    "\n",
    "We have already met the discrete Bernoulli$(\\theta)$ RV.  Remember, that if we have an event $A$ with $P(A) = \\theta$, then a Bernoulli$(\\theta)$ RV $X$ takes the value $1$ if \"$A$ occurs\" with probability $\\theta$ and $0$ if \"$A$ does not occur\" with probability $1-\\theta$. \n",
    "\n",
    "In other words, the indicator function $\\mathbf{1}_A$ of \"$A$ occurs\" with probability $\\theta$ is the Bernoulli$(\\theta)$ RV. \n",
    "\n",
    "For example, flip a fair coin.  \n",
    "Consider the event that it turns up heads.  Since the coin is fair, the probability of this event $\\theta$ is $\\frac{1}{2}$.  If we define an RV $X$ that takes the value 1 if the coin turns up heads (\"event coin turns up heads occurs\") and 0 otherwise, then we have a Bernoulli$(\\theta = \\frac{1}{2})$ RV.  \n",
    "\n",
    "We all saw that given a parameter $\\theta \\in [0,1]$, the probability mass function (PMF) for the Bernoulli$(\\theta)$ RV $X$ is:\n",
    "\n",
    "$$\n",
    "f(x;\\theta) = \\theta^x (1-\\theta)^{1-x} \\mathbf{1}_{\\{0,1\\}}(x) =\n",
    "\\begin{cases}\n",
    "\\theta & \\text{if } x=1 \\ ,\\\\\n",
    "1-\\theta & \\text{if } x=0 \\ ,\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "and its DF is:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "F(x;\\theta) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } 1 \\le x \\ ,\\\\\n",
    "1-\\theta & \\text{if } 0 \\le x < 1 \\ ,\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Now let's look at some expectations:  the population mean and variance of an RV $X \\thicksim$  Bernoulli$(\\theta)$. \n",
    "\n",
    "Because $X$ is a discrete RV, our expectations use sums rather than integrals.\n",
    "\n",
    "The first moment or expectation is: \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl} \n",
    "E(X) & = & \\displaystyle\\sum_{x=0}^{1}xf(x;\\theta) \\\\ &=& (0 \\times (1-\\theta)) + (1 \\times \\theta)\\\\ &=& 0 + \\theta\\\\ &=& \\theta\\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "The second moment is: \n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl} \n",
    "E(X^2) &=& \\displaystyle\\sum_{x=0}^{1}x^2f(x;\\theta) \\\\ \n",
    "&=& (0^2 \\times (1-\\theta)) + (1^2 \\times \\theta)\\\\ &=& 0 + \\theta\\\\ &=& \\theta\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "The variance is: \n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl} \n",
    "V(X) &=& E(X^2) - \\left(E(X)\\right)^2\\\\ &=& \\theta - \\theta^2\\\\ &=& \\theta(1-\\theta) \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "We can see that $E(X)$ and $V(X)$ will vary with the parameter $\\theta$.  This is why we subscript $E$ and $V$ with $\\theta$, to emphasise that the values depend on the parameter.\n",
    "\n",
    "$$E_{\\theta}(X) = \\theta$$\n",
    "\n",
    "$$V_{\\theta}(X) = \\theta(1-\\theta)$$\n",
    "\n",
    "We can do a simple plot to see how $E_{\\theta}(X)$ and $V_{\\theta}(X)$ vary with $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulliPopMean(th):\n",
    "    '''A function to find the population mean for an RV distributed Bernoulli(theta).\n",
    "    \n",
    "    parameter th is the distribution parameter theta.'''\n",
    "\n",
    "    return th\n",
    "    \n",
    "def bernoulliPopVariance(th):\n",
    "    '''A function to find the population variance for an RV distributed Bernoulli(theta).\n",
    "    \n",
    "    parameter th is the distribution parameter theta.'''\n",
    "    \n",
    "    return th*(1-th)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "theta = np.linspace(0,1,100)\n",
    "plt.plot(theta,bernoulliPopMean(theta))\n",
    "plt.plot(theta,bernoulliPopVariance(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the variance is maximized at $\\theta=\\frac{1}{2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The population mean and variance of the Uniform$(0,1)$ RV\n",
    "\n",
    "Now let's look at the the population mean and variance of a continuous RV $X \\thicksim$ Uniform$(0,1)$. \n",
    "\n",
    "Because $X$ is a continuous RV, our expectations use integrals.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl} E(X) &=&\\int_{x=0}^1 x f(x)\\, dx\\\\ &=& \\int_{x=0}^1 x \\ 1 \\, dx\\\\ &=& \\frac{1}{2} \\left( x^2 \\right]_{x=0}^{x=1}\\\\ &=& \\frac{1}{2} \\left( 1-0 \\right)\\\\ &=& \\frac{1}{2} \\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl} E(X^2) &=& \\int_{x=0}^1 x^2 f(x)\\, dx \\\\ &=& \\int_{x=0}^1 x^2 \\ 1 \\, dx\\\\ &=& \\frac{1}{3} \\left[ x^3 \\right]_{x=0}^{x=1}\\\\ &=& \\frac{1}{3} \\left( 1-0 \\right)\\\\ &=& \\frac{1}{3}\\\\ \\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl} V(X) &=& E(X^2) - \\left(E(X)\\right)^2\\\\ &=&\\frac{1}{3} - \\left( \\frac{1}{2} \\right)^2\\\\ &=& \\frac{1}{3} - \\frac{1}{4}\\\\ &=& \\frac{1}{12} \\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Winnings on Average (Law of the unconscious statistician)\n",
    "\n",
    "Think about playing a game where we draw $x \\thicksim X$ and I pay you $r(x)$ ($r$ is some reward function, a function of $x$ that says what your reward is when $x$ is drawn).  Then, your average winnings from the game is the sum (or integral), over all the possible values of $x$, of $r(x) \\times$ the chance that $X=x$.\n",
    "\n",
    "Put formally, if $Y= r(X)$, then\n",
    "\n",
    "$$E(Y) = E(r(X)) = \\int r(x) \\,dF(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showURL(url, ht=500):\n",
    "    \"\"\"Return an IFrame of the url to show in notebook with height ht\"\"\"\n",
    "    from IPython.display import IFrame\n",
    "    return IFrame(url, width='95%', height=ht) \n",
    "showURL('https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician',600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability is an Expectation\n",
    "\n",
    "Remember when we first talked about the probability of some event $A$, we talked about the idea of the probability of $A$ as the long term relative frequency of $A$? \n",
    "\n",
    "Now consider some event $A$ and  a reward function $r(x) = \\mathbf{1}_A(x)$.\n",
    "\n",
    "Recall that $\\mathbf{1}_A(x) = 1$ if $x \\in A$ and $\\mathbf{1}_A(x) = 0$ if $x \\notin A$: the reward is 1 if $x \\in A$ and 0 otherwise.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl} \n",
    "    \\text{If } X \\text{ is continuous } E(\\mathbf{1}_A(X)) &=& \\int \\mathbf{1}_A(x)\\, dF(x)\\\\ \n",
    "    &=& \\int_A f(x)\\, dx\\\\ \n",
    "    &=& P(X \\in A) = P(A)\\\\ \n",
    "    \\text{If } X \\text{ is discrete } E(\\mathbf{1}_A(X)) &=& \\mathbf{1}_A(x)\\, f(x)\\\\ \n",
    "    &=& \\sum_{x \\in A} f(x)\\\\ &=& P(X \\in A) = P(A) \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "This says that probability is a special case of expectation: the probability of $A$ is the expectation that $A$ will occur.\n",
    "\n",
    "Take a Uniform$(0,1)$ RV $X$.  What would you say the probability that an observation of this random variable is $\\le 0.5$ is, ie what is $P(X \\le 0.5)$?\n",
    "\n",
    "Let's simulate some observations for us and look at the relative frequency of observations $\\le 0.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform(0,1) \n",
    "# remember calling this each time changes the outcome - reevaluate this cell and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countObOfInterest = 0    # variable to count observations of interest\n",
    "numberOfObs = 100       # variable to control how many observations we simulate\n",
    "obOfInterest = 0.5        # variable for observation of interest\n",
    "for i in range(numberOfObs): # loop to simulate observations\n",
    "    if uniform(0,1) <= obOfInterest:    # conditional statement to check observation\n",
    "        countObOfInterest += 1    # accumulate count of observation of interest\n",
    "        \n",
    "print (\"The relative frequency of x <=\", obOfInterest, \\\n",
    "      \" was\", countObOfInterest/numberOfObs)   # just formatting out print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we could look at a similar simulation for a discrete RV, say a Bernoulli$(\\frac{1}{2})$ RV. \n",
    "\n",
    "Another way of thinking about the Bernoulli$(\\frac{1}{2})$ RV is that it has a discrete uniform distribution over $\\{0,1\\}$.  It can take on a finite number of values (0 and 1 only) and the probabilities of observing either of these two values are are equal.   \n",
    "\n",
    "This could be modelling the event that we get a head when we throw a fair coin. For this we'll use the `randint(0,1)` function to simulate the observed value of our RV $X$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randint(0,1) # try again and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countObOfInterest = 0    # variable to count observations of interest\n",
    "numberOfObs = 1000        # variable to control how many observations we simulate\n",
    "obOfInterest = 1        # variable for observation of interest\n",
    "for i in range(numberOfObs): # loop to simulate observations\n",
    "    if randint(0,1) == obOfInterest:    # conditional statement to check observation\n",
    "        countObOfInterest += 1    # accumulate count of observation of interest\n",
    "        \n",
    "print (\"The relative frequency of x ==\", obOfInterest, \\\n",
    "      \" was\", countObOfInterest/numberOfObs)   # just formatting out print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showURL(\"https://en.wikipedia.org/wiki/Regular_polygon\",300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps it is simpler to start looking at multivariate random variables from the observations, and then build empirical versions of the concepts. So lets construct first a set of observations for the first component, and then based on the first component we will construct the second component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1_obs = [randint(0,1) for i in range(10)]\n",
    "X_2_obs = [randint(0,1)+x for x in X_1_obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the JDF (Joint Distribution Function) for the random variable for which the first component is `X_1_obs` and the second component is `X_2_obs`, there are 6 possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def F_X_12(x):\n",
    "    TF10 = {True: 1, False: 0}\n",
    "    return np.mean([TF10[(x1 <= x[0]) and (x2 <= x[1])] for x1,x2 in zip (X_1_obs,X_2_obs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x1 in range(0,2):\n",
    "    for x2 in range(0,3):\n",
    "        print(F_X_12((x1,x2)),end=',\\t')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical object for the above is a multivariate random variable $X = (X_1,X_2): \\Omega \\to R^2$, and we just defined the empirical version of the Joint Distribution Function\n",
    "\n",
    "$$\n",
    "    F(x) = P(X \\leq x) = P(X_1 \\leq x_1 \\text{ and } X_2 \\leq x_2)\n",
    "$$\n",
    "\n",
    "That is, $F: R^2 \\to [0,1]$.\n",
    "\n",
    "The next natural thing to look at is the marginal distribution. It is defined as\n",
    "\n",
    "$$\n",
    "    F_{X_i}(x_i) = P(X_i \\leq x_i) \n",
    "$$\n",
    "\n",
    "In our case this just means that if we want the marginal for, say, $X_2$ we can just ignore $X_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Utils import makeEDF\n",
    "makeEDF(X_2_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is the last line in the JDF. If we want the marginal for $X_1$ we can take the last column of the JDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next natural thing to look at here is the conditional distribution, so lets try something simple, lets condition on $X_2 = 1$.\n",
    "\n",
    "From the notes, for two random variables $X$ and $Y$ we have the definition of conditional distribution\n",
    "\n",
    "$$\n",
    "    F_{X \\mid Y}(x \\mid A) := \\frac{P(X \\leq x, Y \\in A)}{P(Y \\in A)}\n",
    "$$\n",
    "\n",
    "If we rewrite this for what we want to do (condition on $X_2 = 1$) we get\n",
    "\n",
    "$$\n",
    "    F_{X_1 \\mid Y_2}(x_1 \\mid X_2 = 1) := \\frac{P(X_1 \\leq x_1, X_2 = 1)}{P(X_2 = 1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_1 can only take values 0 or 1 so it is enough to look at those\n",
    "topRow = np.mean([x1 <= 0 and x2 == 1 for x1,x2 in zip(X_1_obs,X_2_obs)])\n",
    "bottomRow = np.mean([x2 == 1 for x1,x2 in zip(X_1_obs,X_2_obs)])\n",
    "print(topRow/bottomRow)\n",
    "\n",
    "topRow = np.mean([x1 <= 1 and x2 == 1 for x1,x2 in zip(X_1_obs,X_2_obs)])\n",
    "bottomRow = np.mean([x2 == 1 for x1,x2 in zip(X_1_obs,X_2_obs)])\n",
    "print(topRow/bottomRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look a this problem is to only look at $X_1$ when $X_2 = 1$, we can do that by filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1_when_X_2_1 = [x1 for x1,x2 in zip(X_1_obs,X_2_obs) if x2 == 1]\n",
    "X_1_when_X_2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is easy to see what the conditional distribution is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "lx_course_instance": "2022",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
