{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science](http://datascience-intro.github.io/1MS041-2022/)    \n",
    "## 1MS041, 2022 \n",
    "&copy;2022 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation - Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters, Models and Real Life\n",
    "\n",
    "What do we mean by parametric estimation?  In parametric estimation, we assume that the data comes from a particular type of probability distribution and we try to estimate the parameters of that distribution.  What are parameters, in a statistical sense?  Remember the $Bernoulli(\\theta)$ random variable?  A $Bernoulli$ distribution has one parameter, usually denoted as $\\theta$.  We talked about modelling events such as the outcome of a toss of a coin as using the $Bernoulli(\\theta)$ random variable.  If the coin is fair then, in our model, $\\theta = \\frac{1}{2}$.  If the random variable $X$ takes value 1 when the fair coin lands heads, then we model $P(X = 1) = \\frac{1}{2}$.\n",
    "\n",
    "When we speak about the probability of observing events such as the outcome of a toss of a coin, we are assuming some kind of model.  In the case of a coin, the model is a $Bernoulli$ RV. This model would have one parameter,  the probability of the coin landing heads.\n",
    "\n",
    "When we introduced the $Exponential$ distribution, we talked about the $Exponential(\\lambda$) RV, or the $Exponential$ parameterised by $\\lambda$.  Distributions can be parameterised by more than one quantity.  We have already met the $Uniform(\\theta_1, \\theta_2)$ - this has two parameters, $\\theta_1$ and $\\theta_2$.  Another distribution you may be familiar with, although we have not discussed it in this course, is the Normal distribution which is parameterised by $\\mu$ and $\\sigma$.   The symbols like $\\theta$, $\\lambda$, $\\mu$, $\\sigma$ are conventionally used for the parameters of these distributions.  It is useful to  become familiar with these conventions (see for example Wikipedia on the Exponential  or Normal)\n",
    "\n",
    "There are many applications of computational statistics which involve models of real life events, and it is not enough to say \"this can be modeled with a $Bernoulli$ RV\", or \"Orbiter bus inter-arrival times can be modelled with an $Exponential$ RV\".  We also have to choose parameters for our models.\n",
    "\n",
    "We also remind ourselves that the probabilty density function (probability mass function for a discrete random variable) and distribution function depend on the parameters when we write them.  In the case of the $Bernoulli$, for example, the probability mass function is denoted by  $f(x;\\theta)$.  \n",
    "\n",
    "In real life, we are usually not trying to do \"textbook\" examples like calculating the probability of an event given a distribution and parameter value(s).  We may be able to see the outcomes of a process, but we can never know exactly what that process is, all we can do is try to find a useful model for it.  We are trying to use the information available to us in the form of observations or data to make our models, including guessing/estimating values for the model parameters.   We have to turn our thinking around and focus on what the data can tell us about the model.  In particular, in this course, we focus on what the data can tell us about model parameters - parametric estimation.   Now is a good time to reflect on these words of the renowned Statisticians:\n",
    "\n",
    "> All models are wrong, but some are useful --- George Edward Pelham Box\n",
    "\n",
    "> The only math I did not use is the one I did not know -- Lucien Le Cam\n",
    "\n",
    "### The Likelihood Function\n",
    "\n",
    "Likelihood, as we said above, is a fundamental concept in statistical inference (\"inference\" - making inferences from observations, making guesses based on information in data).\n",
    "\n",
    "In informal terms, likelihood is \"the likelihood of the parameters for the observed data\".  More formally, *likelihood is a function of the parameter and is proportional to the conditional probability of the data given the parameter*. \n",
    "\n",
    "We can talk about a likelihood function where the domain of the likelihood function is all the possible values for the parameters (remember, a function is a mapping from a domain to a range): the likelihood function is a mapping from possible values for the parameters to the likelihood of those parameters for the data.  \n",
    "\n",
    "The likelihood function of $\\theta$ based on $n$ observations $x_1, x_2, \\ldots, x_n$ is denoted $L_n(\\theta)$.  We have said that it is a mapping from \"all possible values for the parameters\", i.e. all possible values for $\\theta$, to the likelihood of those parameters given the data $x_1, x_2, \\ldots, x_n$.  In our formal notation, if we know that $\\theta$ must be somewhere in the parameter space ${\\mathbf{\\Theta}}$, then $L_n(\\theta)$ is a mapping from $\\mathbf{\\Theta}$ to the real numbers $\\mathbb{R}$:\n",
    "\n",
    "$$L_n(\\theta): \\mathbf{\\Theta} \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "For example, in the case of a $Bernoulli(\\theta)$ RV, we know that the parameter $\\theta$ must be between $0$ and $1$, or $\\theta \\in [0,1]$.  In the case of an $Exponential(\\lambda)$ random variable parameterised by $\\lambda$, we know $\\lambda > 0$, i.e., $\\lambda \\in \\mathbf{\\Lambda} = (0,\\infty)$. \n",
    "\n",
    "NOTE: $\\Lambda$ is the Greek upper-case of $\\lambda$, which as usual we reserve for random variables taking values in $\\mathbf{\\Lambda}$. These distinctions are not too impotant for this course, and we will keep the notation light here, but when you learn more advanced concepts in statistical machine learning and Bayesian inference, it will become very important to keep the notations clear between samples, random variables and the space where they take their values for data, statistics and parameters (which themselves become random variables in advanced courses you may take in a couple years).\n",
    "\n",
    "We will focus on the likelihood function for independent and identically distributed (IID) random variables.\n",
    "\n",
    "Suppose we have $X_1,X_2,\\ldots,X_n$ as $n$ independent random variables and they are all identically distributed with $f(x;\\theta)$.   We would write this as $X_1,X_2,\\ldots,X_n \\overset{IID}{\\sim} f(x;\\theta)$. Here $(X_1,X_2,\\ldots,X_n)$ is an $\\mathbb{R}^n$-valued random variable (sometimes called a random vector). Then, \n",
    "\n",
    "$$X_1,X_2,\\ldots,X_n \\overset{IID}{\\sim} f(x;\\theta)$$\n",
    "\n",
    "merely means, due to independent and identical distribution among all $X_1,X_2,\\ldots,X_n$, that the $\\mathbb{R}^n$-valued random variable $(X_1,X_2,\\ldots,X_n)$ has joint density:\n",
    "\n",
    "$$f(x_1,x_2,\\ldots,x_n; \\theta) = f(x_1;\\theta)\\times f(x_2;\\theta) \\times \\cdots \\times f(x_n;\\theta) = \\prod_{i=1}^n f(x_i;\\theta) $$\n",
    "\n",
    "Thus, $f(x_1,x_2,\\ldots,x_n; \\theta)$ is termed the joint density of the data $(X_1, X_2, \\ldots, X_n)$ for a given $\\theta$.\n",
    "\n",
    "In conclusion, when $X_1,X_2,\\ldots,X_n \\overset{IID}{\\sim} f(x;\\theta)$ the joint density $f(x_1, x_2, \\ldots, x_n; \\theta)$ is the product of the individual densities $\\displaystyle \\prod_{i=1}^n f(x_i ; \\theta)$. \n",
    "\n",
    "\n",
    "This implies that the likeilihood function $L_n(\\theta)$, which formally also depends on the observed data, is:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "L_n(\\theta) &:= L_n(\\theta; x_1,x_2,\\ldots,x_n) = L_n(\\theta; \\text{ data })\\\\\n",
    "&\\propto P(\\text{ data } | \\theta)\\\\\n",
    "&= f(x_1,x_2,\\ldots,x_n; \\theta)\\\\\n",
    "&= f(x_1;\\theta)\\,f(x_2;\\theta) \\ldots f(x_n;\\theta) \\\\\n",
    "&=: \\prod_{i=1}^n f(x_i ; \\theta)\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "So when $X_1,X_2,\\ldots,X_n \\overset{IID}{\\sim} f(x;\\theta)$, \n",
    "\n",
    "$$\n",
    "\\boxed{L_n(\\theta) := \\prod_{i=1}^n f(x_i ; \\theta)}\n",
    "$$\n",
    " \n",
    "\n",
    "#### The likelihood Function for the $Bernoulli(\\theta)$ RV\n",
    "\n",
    "We can make all this theory a little more real by considering the $Bernoulli$ RV.  In the last worksheet, we wrote function to be able to simulate samples from a $Bernoulli(\\theta)$ RV given some value for the parameter $\\theta$ and the number of samples required.  Suppose we used this function, with a small adaptation, to simulate some samples now using $\\theta^*$ - but there is a catch:  you don't know what the value of $\\theta^*$ is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare to simulate data from a secret $\\theta^*$\n",
    "\n",
    "Just place cursor on next hidden cell hit 'Run' button to evaluate it - its generating our simulated data from a secret $theta^*$ that we will not see by clicking on _here_ below. It has a function `bernoulliSampleSecretTheta(n)` that will produce $n$ IID $Bernoulli(\\theta^*)$ samples for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this can be hidden after evaluation to save space on display\n",
    "from random import random\n",
    "from math import floor\n",
    "from random import seed as set_random_seed\n",
    "\n",
    "def bernoulliFInverse(u, theta):\n",
    "    '''A function to evaluate the inverse CDF of a bernoulli.\n",
    "    \n",
    "    Param u is the value to evaluate the inverse CDF at.\n",
    "    Param theta is the distribution parameters.\n",
    "    Returns inverse CDF under theta evaluated at u'''\n",
    "    \n",
    "    return floor(u + theta)\n",
    "    \n",
    "def bernoulliSampleSecretTheta(n, theta=0.30, mySeed=30):\n",
    "    '''A function to simulate samples from a bernoulli distribution.\n",
    "    \n",
    "    Param n is the number of samples to simulate.\n",
    "    Param theta is the bernoulli distribution parameter.\n",
    "    Param mySeed is a seed for the random number generator, defaulting to None.\n",
    "    Returns a simulated Bernoulli sample as a list.'''\n",
    "    set_random_seed(mySeed)\n",
    "    us = [random() for i in range(n)]\n",
    "    set_random_seed(None)\n",
    "    # use bernoulliFInverse in a list comprehension\n",
    "    return [bernoulliFInverse(u, theta) for u in us] \n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    } else {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "} \n",
    "\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "\n",
    "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Press Ctrl+Enter to evaluate this cell \n",
    "## and obtain our data, i.e., sample of 10 Bernoulli(theta*) RVs\n",
    "bSample = bernoulliSampleSecretTheta(10)\n",
    "print( bSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate the hidden cell above and hide it immediately - its generating our simulated data from a secret theta*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have is $X_1,X_2,\\ldots,X_n \\overset{IID}{\\sim} Bernoulli(\\theta^*)$ where $\\theta^* \\in [0,1]$ is the true, but unknown value (assuming you have not peeked into click `here` above!) of the parameter $\\theta$ responsible for producing all those observations in `bSample`.\n",
    "\n",
    "Recall that the $Bernoulli(\\theta)$ RV $X$ has probability mass function (PMF), for $x \\in \\{0, 1\\}$, $f(x;\\theta)$: \n",
    "\n",
    "$$\n",
    "f(x;\\theta)= \\theta^x (1-\\theta)^{1-x} \n",
    "=\\begin{cases}\n",
    "\\theta & \\text{ if } \\ x=1,\\\\\n",
    "1-\\theta &\\text{  if } \\ x=0,\\\\\n",
    "0 & \\text{ otherwise} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So, for $x_1, x_2, \\ldots, x_n \\in \\{0,1\\}$, the joint density of $n$ IID $Bernoulli(\\theta)$ RVs is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "f(x_1,x_2,\\ldots,x_n; \\theta) \n",
    ":= \\displaystyle\\prod_{i=1}^n f(x_i ; \\theta) \n",
    "& = & \\prod_{i=1}^n\\theta^{x_i}(1-\\theta)^{1-x_i}\\\\ \n",
    "& = & \\theta^{\\sum_{i=1}^n x_i} (1-\\theta)^{\\left(n - \\sum_{i=1}^n x_i\\right)} \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$\\sum_{i=1}^n x_i$ is a bit of a mouthful, so lets summarise this as $t_n = \\displaystyle\\sum_{i=1}^n x_i$\n",
    "\n",
    "We can use $t_n$ to make our likelihood function a little more friendly:\n",
    "\n",
    "$$L_n(\\theta) = \\theta^{t_n}(1-\\theta)^{(n-t_n)}$$\n",
    "\n",
    "What we have actually done is to *define a statistic of the data*.  Remember that a statistic is a function of the data.  We will call our statistic (note the big $T$) $T_n$.  The $n$ subscript reminds us that it is a function of $n$ observations or data points $X_1,X_2,\\ldots,X_n$. \n",
    "\n",
    "$T_n$ is a function of the data, a mapping from the data space $\\mathbb{X} = \\{0,1\\}^n$ to the space $\\mathbb{T}_n$:\n",
    "\n",
    "$$\n",
    "T_n(X_1, \\ldots, X_n) = \\displaystyle \\sum_{i=1}^n X_i : \\mathbb{X} \\rightarrow \\mathbb{T}_n\n",
    "$$\n",
    "\n",
    "If you are wondering what the space $\\mathbb{T}_n$ is for the $Bernoulli$, think about the range of possible values of $\\displaystyle\\sum_{i=1}^n X_i$ when each $X_i$ can only be 0 or 1..... Now, do you agree that $\\mathbb{T}_n=\\{0,1,\\ldots,n\\}$? If so, observe that the size of the data space $\\mathbb{X}_n$ is $2^n$ but that of the statistic $\\mathbb{T}_n$ is just $n+1$. The natural question now is if we have lost any information about the parameter $\\theta$ that was in the data when summarising the data by this statistic? Such questions of so called *[sufficiency](https://en.wikipedia.org/wiki/Sufficient_statistic) of the statistic for the probability model parameterised by $\\theta$ that is assumed to be generating the data* are the starting point for a more mathematical statistical path one can take. We will not delve into this here.\n",
    "\n",
    "**In conclusion**, we have some actual observations or data points or just sample $(x_1, \\ldots, x_n)$, such that:\n",
    "\n",
    "- $(x_1, \\ldots, x_n)$ as a realisation of $\\mathbb{X}$-valued random variable $(X_1,X_2,\\ldots,X_n)$ from $n$ IID $Bernoulli(\\theta)$ trials\n",
    "  - with joint density $f(x_1,x_2,\\ldots,x_n; \\theta): \\mathbf{X} \\to \\mathbb{R}$ \n",
    "- and corresponding to this we have a realisation of our statistic $T(X_1,X_2,\\ldots,X_n)=\\sum_{i=1}^n X_i : \\mathbb{X} \\to \\{0,1,\\ldots,n\\}$:\n",
    "  - $T_n(x_1, \\ldots, x_n) = t_n = \\displaystyle\\sum_{i=1}^n x_i$\n",
    "- and finally we can express the likelihood function as $L_n(\\theta) = \\theta^{t_n}(1-\\theta)^{(n-t_n)}$\n",
    "\n",
    "We can easily use Python to calculate $t_n$ for us, using the sum function.  For example, for the small sample of 20 simulated Bernoulli observations above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = sum(bSample)\n",
    "tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write ourselves a Python function to calculate the likelihood of a specified value of $\\theta$ given $n$ and $t_n$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihoodBernoulli(theta, n, tStatistic):\n",
    "    '''Bernoulli likelihood function.\n",
    "    theta in [0,1] is the theta to evaluate the likelihood at.\n",
    "    n is the number of observations.\n",
    "    tStatistic is the sum of the n Bernoulli observations.\n",
    "    return a value for the likelihood of theta given the n observations and tStatistic.'''\n",
    "    retValue = 0 # default return value\n",
    "    if (theta >= 0 and theta <= 1): # check on theta\n",
    "        mpfrTheta = theta\n",
    "        retValue = (mpfrTheta**tStatistic)*(1-mpfrTheta)**(n-tStatistic)\n",
    "    return retValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YouTry in class\n",
    "\n",
    "You should be able to understand what the `likelihoodBernoulli` function is doing and be able to write this kind of Python function for yourselves.  Why do we need to check that the value for `theta` passed to the function is between 0 and 1?  How does the function deal with a situation where it is asked to evaluate a likelihood for `theta < 0` or `theta > 1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(end of You Try)\n",
    "\n",
    "---\n",
    "\n",
    "Let's look at a very simple situation where we have one observation ($n=1$) and it is a 0.  What is the realisation of $T_1$, i.e., $t_1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulliSample0 = [0]\n",
    "tn = sum(bernoulliSample0)\n",
    "tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YouTry in class if you are bored or at home\n",
    "\n",
    "Try going back to the $Bernoulli$ likelihood function $L_n(\\theta) = \\theta^{t_n}(1-\\theta)^{(n-t_n)}$ to calculate the likelihood of $\\theta =0$ without using the Pythob function. Think about what the likelihood function is doing (the mathematical derivation for the $Bernoulli$ case was already scribed above, but you need to be able to do such derivations on your own in the exam or in assignments when I give you another distribution that the data is independent and identically distributed from). \n",
    "\n",
    "When you have done that, check that you get the same answer using our `likelihoodBernoulli` Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryTheta0 = 0 # a value of theta to find the likelihood for\n",
    "n = len(bernoulliSample0) # find n as the length of the sample list\n",
    "tn = sum(bernoulliSample0) # find tn as the sum of the samples\n",
    "## calculate the likelihood of theta=tryTheta0=0\n",
    "likelihoodBernoulli(tryTheta0, n, tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about $\\theta = 1$?  What is the likelihood of $\\theta = 1$ when we have observed 0?  Think back to what the $\\theta$ parameter means in a $Bernoulli$ distribution:\n",
    "\n",
    "The $Bernoulli(\\theta)$ RV $X$ has probability mass function (PMF), for $x \\in \\{0, 1\\}$, $f(x;\\theta)$: \n",
    "\n",
    "$$\n",
    "f(x;\\theta)= \\theta^x (1-\\theta)^{1-x} =\\begin{cases}\\theta& \\text{ if } \\ x=1,\\\\1-\\theta &\\text{ if } \\ x=0,\\\\0 & \\text{ otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "Remember that the idea behind the likelihood function is \"what is the likelihood of a parameter value given our data?\"\n",
    "\n",
    "When you have worked out the answer using $L_n(\\theta)$, check using our `likelihoodBernoulli` SageMath function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryTheta1 = 1 # a value of theta to find the likelihood for\n",
    "n = len(bernoulliSample0) # find n as the length of the sample list\n",
    "tn = sum(bernoulliSample0) # find tn as the sum of the samples\n",
    "## calculate the likelihood of theta=tryTheta0=0\n",
    "likelihoodBernoulli(tryTheta1, n, tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about $\\theta = \\frac{1}{2}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryTheta1 = 0.5 # a value of theta to find the likelihood for\n",
    "n = len(bernoulliSample0) # find n as the length of the sample list\n",
    "tn = sum(bernoulliSample0) # find tn as the sum of the samples\n",
    "## calculate the likelihood of theta=tryTheta0=0\n",
    "likelihoodBernoulli(tryTheta1, n, tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to sketch the likelihood function over $\\theta \\in [0,1]$ for our one observation of 0.\n",
    "\n",
    "Now, what if instead of observing a 0 in our one-observation case, we had observed a 1?\n",
    "\n",
    "What is our realisation of $T_n$ now?  What is our intuition about the likelihood of $\\theta = 0$? $\\theta = 1$? \n",
    "\n",
    "Again try to sketch the likelihood function for our single observation of 1.\n",
    "\n",
    "We could use `likelihoodBernoulli` and a for loop to calculate the likelihood for some different values of $\\theta$ without repeating code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulliSample1 = [1]\n",
    "n = len(bernoulliSample1) # find n as the length of the sample list\n",
    "tn = sum(bernoulliSample1) # find tn as the sum of the samples\n",
    "import numpy as np\n",
    "for t in np.linspace(0, 1, 10):\n",
    "    # calculate the likelihood of theta=tryTheta0=0\n",
    "    print (\"If we observe\", bernoulliSample1, \\\n",
    "        \"The likelihood of theta=\", t, \" is \", \\\n",
    "        likelihoodBernoulli(t, n, tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we could use a list comprehension to get a list of likelihoods corresponding to the list of possible values of $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ths = np.linspace(0, 1, 10)\n",
    "[likelihoodBernoulli(t,len(bernoulliSample1),sum(bernoulliSample1)) \\\n",
    "     for t in ths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(end of You Try)\n",
    "\n",
    "---\n",
    "\n",
    "Now, we look at a possible sample of $n=2$ observations from a Bernoulli process with unknown $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallBSample = [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is $t_n$, the realisation of the $T_n$ statistic now with $n=2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = sum(smallBSample) # what is tn\n",
    "tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use $L_n(\\theta) = \\theta^{t_n}(1-\\theta)^{(n-t_n)}$ to think about the likelihood of some possible values for $\\theta$ given this data.  Think what the shape of the likelihood function might be. \n",
    "\n",
    "In the visualisation below we have used our `likelihoodBernoulli` function to plot the likelihood function for the cases where we have a single observation 0, a single observation 1, and a small sample of size two with observations: 0, 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulliSample0 = [0] # make sure we know about our samples\n",
    "bernoulliSample1 = [1]\n",
    "smallBSample = [0,1]\n",
    "import matplotlib.pyplot as plt\n",
    "ths = np.linspace(0,1,100) # get some values to plot against\n",
    "plt.plot(ths, [likelihoodBernoulli(t, len(bernoulliSample0),sum(bernoulliSample0)) for t in ths])\n",
    "plt.title(\"One observation, x1 = 0\")\n",
    "plt.show()\n",
    "plt.plot(ths, [likelihoodBernoulli(t, len(bernoulliSample1),sum(bernoulliSample1)) for t in ths])\n",
    "plt.title(\"One observation, x1 = 1\")\n",
    "plt.show()\n",
    "plt.plot(ths, [likelihoodBernoulli(t, len(smallBSample),sum(smallBSample)) for t in ths])\n",
    "plt.title(\"Two observations, x1=0, x2=1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other samples we could get if we draw a sample of size $n=2$ from a $Bernoulli$ RV.  In the visualisation below we plot the likelihood functions for the four unique possible samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallBSample1 = [0,0]\n",
    "smallBSample2 = [1,0]\n",
    "smallBSample3 = [0,1]\n",
    "smallBSample4 = [1,1]\n",
    "listOfSamples = [smallBSample1, smallBSample2, smallBSample3, smallBSample4] # a list of lists\n",
    "fig,ax = plt.subplots(1,4,figsize=(10,5))\n",
    "from pylab import arange\n",
    "ths = arange(0,1.01,0.01) # get some values to plot against\n",
    "l_plots = [] # an empty list of plots\n",
    "for axis,sample in zip(ax,listOfSamples): # go through the list of samples one by one\n",
    "    axis.set_title(\"Sample x1 = \" + str(sample[0]) + \", x2 = \" + str(sample[1]))\n",
    "    axis.plot(ths, [likelihoodBernoulli(t, len(sample),sum(sample)) for t in ths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the shape of the likekihood function depends on the sample:\n",
    "\n",
    "- $(x_1,x_2) \\in \\{ (0,0), (1,0), (0,1), (1,1)\\}$ \n",
    "- and more crucially on the statistic \n",
    "  - $t_2 \\in \\{0,1,2\\}$\n",
    "  \n",
    "In the above plots we are looking at the likelihood for the parameter given the actual data (simulated here, of course for pedagogical reasons).  \n",
    "\n",
    "As usual, you do not have to know how to do these plots or the interactive plot below to do well in the exam or assignments, but you should understand what a list comprehension statement like the following:\n",
    "\n",
    "> `[likelihoodBernoulli(t,len(smallBSample),sum(smallBSample)) for t in ths]`\n",
    "\n",
    "is doing, when `ths = arange(0, 1.01, 0.01)` are some possible values for $\\theta \\in [0,1]$.\n",
    "\n",
    "What happens as we increase the sample size $n$?  In the interactive plot below, we use our `bernoulliSampleSecretTheta` to simulate samples of size $n$ with an unknown $\\theta^*$.  You can have a look at the effect of increasing $n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "@interact\n",
    "def _(n=IntSlider(1,1,500,25)):\n",
    "    '''Interactive function to plot the bernoulli likelihood for different n.'''\n",
    "    if n > 0:\n",
    "        thisBSample = bernoulliSampleSecretTheta(n) # make sample\n",
    "        n = len(thisBSample) # what is n\n",
    "        tn = sum(thisBSample)\n",
    "        print (\"Likelihood function for n = \", n , \" and tn = \", tn)\n",
    "        from pylab import arange\n",
    "        ths = arange(0,1,0.001) # get some values to plot against\n",
    "        plt.plot(ths, [likelihoodBernoulli(t, n, tn) for t in ths])\n",
    "    else:\n",
    "        print( \"n must be greater than 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that for each $n$ you try, the shape of the likelihood function will depend on the $t_n$ for the sample simulated by `bernoulliSampleSecretTheta`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Log-likelihood Function\n",
    "\n",
    "Working with products, as in \n",
    "\n",
    "$$\n",
    "L_n(\\theta) = \\displaystyle \\prod_{i=1}^n f(x_i;\\theta) \\quad \\text{,  where  } X_1, X_2, \\ldots, X_n \\overset{IID} \\sim f(x_i;\\theta)\n",
    "$$ \n",
    "\n",
    "(i.e., $n$ independent and identically distributed random variables), can be inconvenient. Taking logs can be useful here.  The log-likelihood function for some parameter $\\theta$ is $l_n(\\theta)$ and it is literally the log of the likelihood function $L_n(\\theta)$:\n",
    "\n",
    "$$l_n(\\theta) := log(L_n(\\theta))$$\n",
    "\n",
    "You will probably recall from pre-calculus that $\\log(a \\times b) = \\log(a) + \\log(b)$.\n",
    "\n",
    "In SageMath, using the `log` function without specifying a base gives the natural logarithm (logarithm to base $e$) of a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "a = 5.0\n",
    "b = 6.0\n",
    "log(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(a) + log(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment next line to see docs of `log`\n",
    "#?log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SageMath `log` function provides a default parameter value of `None` for the base, and if the base is `None` the natural log is calculated.  If you specify a value for the base when you use the `log` function, you will get the logarithm using this base (default parameter values and `Non`e were discussed in the last notebook:  go back there in your own time if you need to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(a, 10) # log to base 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generalise this into a useful trick (of course most of you must have seen this in high-school, but just in case!):  \n",
    "\n",
    "**the log of products is the sum of logs**\n",
    "\n",
    "$$\\log\\left(\\displaystyle\\prod_{j=1}^n y_j\\right) = \\displaystyle\\sum_{j=1}^n\\left(\\log(y_j)\\right)$$\n",
    "\n",
    "So, if we have $n$ IID (independent and identically distributed) random variables, the log-likelihood function is\n",
    "\n",
    "$$\n",
    "X_1,X_2,\\ldots,X_n \\overset{IID}{\\sim} f(x;\\theta) \\Longrightarrow\n",
    "l_n(\\theta) = \\log(L_n(\\theta)) := \\log\\left(\\prod_{i=1}^n f(x_i ; \\theta)\\right) = \\sum_{i=1}^n \\log(f(x_i; \\theta))\n",
    "$$\n",
    "\n",
    "In the case of $X_1, X_2, \\ldots, X_n \\overset{IID}{\\sim} Bernoulli(\\theta)$, $n$ independent and identically distributed $Bernoulli$ RVs,\n",
    "\n",
    "$$\n",
    "l_n(\\theta) = \\log(L_n(\\theta)) = \\log\\left(\\theta^{t_n}(1-\\theta)^{(n-t_n)}\\right) = t_n\\log(\\theta) + (n-t_n)\\log(1-\\theta)\n",
    "$$\n",
    "\n",
    "Here we are using the fact that $\\log(a^c) = c\\log(a)$. \n",
    "\n",
    "If this is not familiar to you, consider $\\log(a^2) = \\log(a \\times a) = \\log(a) + \\log(a) = 2\\log(a)$\n",
    "\n",
    "and then think what happens if you have $\\log(a^3) = \\log(a \\times a^2) = \\log(a) +\\ log(a^2) = \\log(a) + 2log(a) = 3\\log(a)$, etc etc.\n",
    "\n",
    "#### YouTry in class\n",
    "\n",
    "Write down for yourself the steps to prove that the log-likelihood function $l_n(\\theta) = t_n\\log(\\theta) + (n-t_n)\\log(1-\\theta)$ for $n$ IID samples from $Bernoulli$ RVs.\n",
    "\n",
    "(end of You Try)\n",
    "\n",
    "---\n",
    "\n",
    "Logarithm is a **monotone function** (also known as a monotonic function).  What does it mean when we say that something is a monotone function?  In words, it means that the function preserves the given order.  For once, putting something as a formula may make it easier to understand than the words.  If $f$ is some monotone function and we have two values $a$ and $b$ in the domain of $f$ (values that $f$ can be applied to) such that $a \\le b$, then $f(a) \\le f(b)$.\n",
    "\n",
    "So, if $a \\le b$, $\\log(a) \\le \\log(b)$:  log preserves order.   If we calculatete the likelihood $L_n$ for two different possible values of $\\theta$, say $\\theta_a$ and $\\theta_b$ and find that $L_n(\\theta_a) \\le L_n(\\theta_b)$, then we know that $l_n(\\theta_a) = \\log(L_n(\\theta_a)) \\le l_n(\\theta_b) = \\log(L_n(\\theta_b))$.\n",
    "\n",
    "We can see this if we adapt our interactive plot for the $Bernoulli$ likelihood function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def _(n=IntSlider(1,1,1000,100)):\n",
    "    '''Interactive function to plot the bernoulli likelihood for different n.'''\n",
    "    fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "    if n > 0:\n",
    "        thisBSample = bernoulliSampleSecretTheta(n) # make sample\n",
    "        n = len(thisBSample) # what is n\n",
    "        tn = sum(thisBSample)\n",
    "        print (\"n = \", n , \" and tn = \", tn)\n",
    "        from pylab import arange\n",
    "        ths = arange(0,1,0.01) # get some values to plot against\n",
    "        liks = [likelihoodBernoulli(t,n,tn) for t in ths]\n",
    "        ax[0].plot(ths, liks)\n",
    "        #p1 = line(zip(ths, liks))\n",
    "        #p1 += text(\"Likelihood function\", (0.5, max(liks)*1.1))\n",
    "        thsForLog = arange(0.01,1,0.01) # fudge to avoid log(0) get some values to plot log against\n",
    "        logliks = [np.log(likelihoodBernoulli(t,n,tn)) for t in thsForLog]\n",
    "        ax[1].plot(thsForLog,logliks)\n",
    "        #p2 = line(zip(thsForLog, logliks), rgbcolor=\"red\")\n",
    "        #p2 += text(\"Log-likelihood function\", (0.5, max(logliks)*0.8), rgbcolor=\"red\")\n",
    "        #show(graphics_array([p1, p2]),figsize=[8,3])\n",
    "    else:\n",
    "        print (\"n must be greater than 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the more samples we have the more negative the log likelihood gets, lets normalize by dividing by $n$, so that we get the empirical risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def _(n=IntSlider(1,1,1000,100)):\n",
    "    '''Interactive function to plot the bernoulli likelihood for different n.'''\n",
    "    fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "    if n > 0:\n",
    "        thisBSample = bernoulliSampleSecretTheta(n) # make sample\n",
    "        n = len(thisBSample) # what is n\n",
    "        tn = sum(thisBSample)\n",
    "        print (\"n = \", n , \" and tn = \", tn)\n",
    "        from pylab import arange\n",
    "        ths = arange(0,1,0.01) # get some values to plot against\n",
    "        liks = [np.power(likelihoodBernoulli(t,n,tn),1/n) for t in ths]\n",
    "        ax[0].plot(ths, liks)\n",
    "        #p1 = line(zip(ths, liks))\n",
    "        #p1 += text(\"Likelihood function\", (0.5, max(liks)*1.1))\n",
    "        thsForLog = arange(0.01,1,0.01) # fudge to avoid log(0) get some values to plot log against\n",
    "        logliks = [(1/n)*np.log(likelihoodBernoulli(t,n,tn)) for t in thsForLog]\n",
    "        ax[1].plot(thsForLog,logliks)\n",
    "        #p2 = line(zip(thsForLog, logliks), rgbcolor=\"red\")\n",
    "        #p2 += text(\"Log-likelihood function\", (0.5, max(logliks)*0.8), rgbcolor=\"red\")\n",
    "        #show(graphics_array([p1, p2]),figsize=[8,3])\n",
    "    else:\n",
    "        print (\"n must be greater than 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimator\n",
    "\n",
    "So now, we know about the **likelihood function**, the function that tells us about the likelihood of parameter values given the data, and we know about the **log-likelihood function**.   How do either of these help us to make an estimate of a parameter value? \n",
    "\n",
    "How about estimating a parameter with the value that maximises the likelihood function?   This is called the **Maximum Likelihood Estimator (MLE)**.\n",
    "\n",
    "And, because log is a monotone function, we know that if some particular value for the unknown parameter maximises the likelihood function, then it will also maximise the log-likelihood function.\n",
    "\n",
    "Formally,\n",
    "\n",
    "Let $(X_1,\\ldots,X_n) \\sim f(x_1,\\ldots,x_n;\\theta^*)$, i.e., let $(X_1,\\ldots,X_n)$ have joint density $f(x_1,\\ldots,x_n;\\theta^*)$ where $\\theta^*$ is the *\"true\"* but possibly unknown parameter value *under our assumed probability model* for the $n$ observations $(x_1,\\ldots,x_n)$.\n",
    "\n",
    "The *maximum likelihood estimator or MLE* denoted by $\\widehat{\\Theta}_n$ of the fixed and possibly unknown true parameter $\\theta^* \\in \\Theta$ is a function that returns the value of $\\theta$ that maximises the likelihood function.\n",
    "\n",
    "As we saw, when we looked at the different possible unique samples of size $n=2$ from a $Bernoulli$ RV, the shape of the likelihood function depends on the data. The maximum likelihood estimator, i.e., the value of $\\theta$ which maximises the likelihood function (or log-likelihood function) is clearly a function of data itself. \n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\widehat{\\Theta}_n := \\widehat{\\Theta}_n(X_1,X_2,\\ldots,X_n) := \\underset{\\theta \\in \\Theta}{argmax} \\, L_n(\\theta) \n",
    "}\n",
    "$$\n",
    "Equivalently, the maximum likelihood estimator is the value of $\\theta$ that maximises the log-likelihood function:\n",
    "$$\n",
    "\\boxed{\n",
    "\\widehat{\\Theta}_n := \\underset{\\theta \\in \\Theta}{argmax} \\, l_n(\\theta)\n",
    "}\n",
    "$$\n",
    "\n",
    "Thus, $\\underset{\\theta \\in \\Theta}{argmax}L_n(\\theta)$ is the value of $\\theta \\in \\Theta$ that maximises $L_n(\\theta)$.   $argmax$ is doing what we try to do by eye when we look at the shape of a likelihood function and try to see which value of $\\theta$ corresponds to the function's highest point. \n",
    "\n",
    "How do we find the value which maximises the likelihood function, or log-likelihood function?  What do we usually do when we want to find the value of a parameter which maximises a function?   We find the turning point(s) of the function by taking the derivative of the function with respect to the parameter (for maximums, we are looking for turning points where the slope of the function changes from positive to negative, which we could check with a second derivative, but let's just concentrate on finding the derivative for the moment).\n",
    "\n",
    "Consider finding the maximum likelihood estimator for $X_1, X_2, \\ldots, X_n \\overset{IID}{\\sim} Bernoulli(\\theta^*)$ ($n$ independent Bernoulli random variables, identically distributed with the same true parameter value $\\theta^*$):\n",
    "\n",
    "We found that the likelihood function $L_n(\\theta) = \\theta^{t_n} (1-\\theta)^{(n-t_n)}$ and the log-likelihood function $l_n(\\theta) = t_n\\log(\\theta) + (n-t_n)\\log(1-\\theta)$.\n",
    "\n",
    "It is much easier to work with the log-likelihood function when we are taking the derivative with respect to $\\theta$:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial \\theta} l_n(\\theta)\n",
    "&=& \\frac{\\partial}{\\partial \\theta}  t_n \\log(\\theta) + \\frac{\\partial}{\\partial \\theta}  (n-t_n) \\log(1-\\theta) \\notag \\\\\n",
    "&=& \\frac{t_n}{\\theta} - \\frac{n-t_n}{1-\\theta} \\notag\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Here, we are using the useful fact that $\\frac{\\partial \\log(\\theta)}{\\partial \\theta} = \\frac{1}{\\theta}$ (and $\\frac{\\partial \\log(1-\\theta)}{\\partial \\theta} = \\frac{-1}{1-\\theta})$\n",
    "\n",
    "Now, set $\\frac{\\partial}{\\partial \\theta} l_n(\\theta)=0$ and solve for $\\theta$ to obtain the maximum likelihood estimate  $\\widehat{\\theta}_n$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta} l_n(\\theta)) = 0 \\iff\n",
    "\\frac{t_n}{\\theta} = \\frac{n-t_n}{1-\\theta} \\iff\n",
    "\\frac{1-\\theta}{\\theta} = \\frac{n-t_n}{t_n} \\iff\n",
    "\\frac{1}{\\theta}-1 = \\frac{n}{t_n}-1 \\iff \\widehat{\\theta}_n = \\frac{t_n}{n}\n",
    "$$\n",
    "\n",
    "What was $t_n$?  \n",
    "\n",
    "$t_n = \\displaystyle \\sum_{i=1}^n x_i$, so we can see that $\\widehat{\\theta}_n = \\frac{1}{n}\\displaystyle\\sum_{i=1}^n x_i$\n",
    "\n",
    "In general, the maximum likelihood estimator as a function of the RVs $X_1, X_2, \\ldots, X_n$  is:\n",
    "\n",
    "$$\n",
    "\\widehat{\\Theta}_n(X_1,X_2,\\ldots,X_n) = \\frac{1}{n}T_n(X_1,X_2,\\ldots,X_n) = \\frac{1}{n} \\sum_{i=1}^n X_i = \\overline{X}_n\n",
    "$$\n",
    "\n",
    "Now, let's look an another version of the interactive plot of the log-likelihood function for a sample of size $n$ from a $Bernoulli$ process with unknown $\\theta^*$, but this time we will show the maximum point on the function and the maximum likelihood estimator (MLE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def _(n=IntSlider(1,1,1000,100)):\n",
    "    '''Interactive function to plot the bernoulli likelihood for different n.'''\n",
    "    if n > 0:\n",
    "        thisBSample = bernoulliSampleSecretTheta(n) # make sample\n",
    "        n = len(thisBSample) # what is n\n",
    "        tn = sum(thisBSample)\n",
    "        from pylab import arange\n",
    "        thsForLog = arange(0.01,1,0.01) # fudge to avoid log(0) get some values to plot log against\n",
    "        logliks = [(1/n)*np.log(likelihoodBernoulli(t,n,tn)) for t in thsForLog]\n",
    "        #p = line(zip(thsForLog, logliks), rgbcolor=\"red\")\n",
    "        plt.plot(thsForLog, logliks, color=\"red\")\n",
    "        MLE = tn/n\n",
    "        plt.scatter(MLE,(1/n)*np.log(likelihoodBernoulli(MLE,n,tn)))\n",
    "        #MLEpt = (MLE, log(likelihoodBernoulli(MLE,n,tn)))\n",
    "        #p += points(MLEpt,rgbcolor=\"red\", pointsize=30)\n",
    "        print (\"Log-likelihood function for n = \", n , \" and tn = \", tn, \": MLE = \",MLE)\n",
    "        #show(p,figsize=[8,3])\n",
    "        plt.show()\n",
    "    else:\n",
    "        print (\"n must be greater than 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YouTry\n",
    "What happens above as you gradually increase $n$?  What do you think the true value $\\theta^*$ is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example : New Zealand Lotto Data\n",
    "\n",
    "Now, we are going to apply what we have learned about maximum likelihood estimates to the the NZ lotto data we looked at earlier in the course.  Specifically, we are interested in whether Ball One is odd or even.   This can be considered as a Bernoulli random variable where the outcome is 1 if the number drawn for Ball One is odd, and 0 if the number drawn is even.   The observed outcomes of the draws for Ball One are modelled as independently and identically distributed (IID) realisations of the $Bernoulli(\\theta^*)$ random variable.    Thus our probability model is:\n",
    "\n",
    "$$X_1,X_2,\\ldots,X_n \\overset{IID}{\\sim} Bernoulli(\\theta^*), \\ \\text{where}, \\ \\theta^* \\in  [0,1] $$\n",
    "\n",
    "We have provided the functions needed for you to access the data, so all you have to do is to evaluate the cell below to get a list of the Ball one data between 1987 and 2008:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These lotto draws of the first ball from NZ Lotto was already downloaded and processed for you\n",
    "listBallOne = [4, 3, 11, 35, 23, 12, 14, 13, 15, 19, 36, 18, 37, 39, 37, 35, 39, 1, 24, 29, 38, 18, 40, 35, \\\n",
    "            12, 7, 14, 23, 21, 35, 14, 32, 19, 2, 1, 34, 39, 29, 7, 20, 2, 40, 28, 4, 30, 34, 20, 37, 9, 24,\\\n",
    "            36, 4, 22, 1, 31, 12, 16, 29, 36, 5, 21, 23, 30, 39, 38, 22, 13, 6, 14, 30, 40, 21, 5, 12, 28, 27,\\\n",
    "            13, 18, 19, 23, 2, 10, 37, 31, 40, 4, 25, 4, 17, 6, 34, 26, 38, 35, 3, 38, 14, 40, 3, 30, 21, 4,\\\n",
    "            24, 34, 27, 14, 25, 18, 21, 1, 25, 39, 18, 40, 18, 11, 5, 37, 33, 26, 29, 26, 36, 33, 18, 32, 3, 1,\\\n",
    "            5, 22, 39, 25, 12, 21, 23, 12, 31, 1, 35, 8, 32, 24, 34, 14, 26, 4, 3, 31, 17, 22, 24, 10, 29, 40,\\\n",
    "            4, 8, 26, 11, 8, 18, 25, 22, 8, 30, 10, 14, 32, 14, 5, 35, 3, 32, 40, 17, 39, 7, 21, 4, 35, 9, 16,\\\n",
    "            30, 30, 11, 28, 22, 38, 5, 16, 27, 16, 23, 22, 1, 27, 32, 30, 24, 32, 29, 11, 3, 26, 19, 22, 25, 3,\\\n",
    "            34, 31, 17, 16, 31, 20, 29, 10, 2, 17, 36, 6, 34, 11, 7, 22, 28, 13, 15, 20, 39, 16, 10, 25, 1, 37,\\\n",
    "            14, 28, 35, 20, 39, 3, 39, 20, 40, 6, 20, 17, 26, 27, 4, 24, 40, 16, 24, 7, 8, 25, 16, 15, 8, 29, 13,\\\n",
    "            16, 39, 2, 24, 24, 23, 24, 37, 39, 40, 5, 11, 13, 6, 24, 1, 5, 7, 15, 38, 3, 35, 10, 22, 19, 3, 21,\\\n",
    "            39, 38, 4, 30, 17, 15, 9, 32, 28, 7, 12, 6, 37, 25, 4, 8, 30, 7, 31, 12, 21, 31, 13, 2, 20, 14, 40,\\\n",
    "            32, 23, 10, 1, 35, 35, 32, 16, 25, 13, 20, 33, 27, 2, 26, 12, 5, 34, 20, 7, 34, 38, 20, 8, 5, 11, 17,\\\n",
    "            10, 36, 34, 1, 36, 6, 7, 37, 22, 33, 7, 32, 18, 8, 1, 37, 25, 35, 29, 23, 11, 19, 7, 21, 30, 23, 12,\\\n",
    "            10, 26, 21, 9, 9, 25, 2, 14, 16, 14, 25, 40, 8, 28, 19, 8, 35, 22, 23, 27, 31, 36, 22, 33, 22, 15, 3,\\\n",
    "            37, 8, 2, 22, 39, 3, 6, 13, 33, 18, 37, 28, 3, 17, 8, 2, 36, 1, 14, 38, 5, 31, 34, 16, 37, 2, 40, 14,\\\n",
    "            16, 21, 40, 5, 21, 24, 24, 38, 26, 38, 33, 20, 25, 7, 33, 12, 22, 34, 34, 20, 38, 12, 20, 7, 28, 26,\\\n",
    "            30, 13, 40, 36, 29, 11, 31, 15, 9, 13, 17, 32, 18, 9, 24, 6, 40, 1, 1, 9, 13, 28, 19, 5, 7, 27, 12,\\\n",
    "            3, 34, 26, 20, 28, 28, 25, 21, 23, 6, 15, 19, 30, 10, 13, 8, 11, 38, 7, 33, 12, 16, 11, 40, 25, 32,\\\n",
    "            34, 1, 32, 31, 33, 15, 39, 9, 25, 39, 30, 35, 20, 34, 3, 30, 17, 24, 20, 15, 10, 25, 6, 39, 19, 20,\\\n",
    "            23, 16, 17, 31, 25, 8, 17, 15, 31, 20, 19, 33, 11, 37, 31, 4, 12, 37, 7, 40, 8, 22, 3, 25, 35, 8, 9,\\\n",
    "            14, 13, 33, 4, 2, 1, 31, 24, 8, 13, 19, 34, 10, 32, 35, 28, 11, 10, 31, 25, 8, 6, 13, 33, 19, 35, 19,\\\n",
    "            8, 21, 10, 40, 36, 16, 27, 31, 1, 18, 36, 40, 18, 37, 18, 24, 33, 34, 31, 6, 10, 24, 8, 7, 24, 27, 12,\\\n",
    "            19, 23, 5, 33, 20, 2, 32, 33, 6, 13, 5, 25, 7, 31, 40, 1, 30, 37, 19, 27, 40, 28, 3, 24, 36, 7, 22,\\\n",
    "            20, 21, 36, 38, 15, 11, 37, 21, 4, 13, 9, 12, 13, 34, 30, 8, 23, 40, 4, 13, 6, 4, 22, 35, 2, 35, 20,\\\n",
    "            9, 28, 9, 13, 33, 19, 5, 38, 24, 18, 37, 10, 25, 25, 31, 3, 13, 25, 35, 1, 36, 21, 3, 22, 23, 7, 6,\\\n",
    "            26, 11, 6, 1, 24, 2, 25, 38, 3, 16, 16, 20, 22, 12, 8, 27, 38, 10, 39, 9, 37, 30, 33, 12, 4, 32, 2,\\\n",
    "            29, 6, 34, 2, 3, 12, 9, 1, 22, 40, 38, 9, 18, 40, 17, 5, 17, 26, 17, 26, 6, 7, 18, 10, 27, 24, 39, 1,\\\n",
    "            3, 26, 38, 2, 12, 5, 7, 38, 2, 8, 30, 35, 18, 19, 29, 37, 5, 27, 35, 40, 14, 25, 15, 20, 32, 22, 9, 1,\\\n",
    "            8, 14, 38, 27, 23, 24, 15, 29, 7, 4, 19, 6, 21, 27, 23, 21, 35, 32, 13, 27, 34, 1, 11, 36, 24, 23, 13,\\\n",
    "            2, 33, 25, 18, 1, 10, 5, 27, 1, 36, 36, 11, 3, 31, 30, 31, 39, 7, 21, 25, 28, 38, 2, 3, 40, 10, 40,\\\n",
    "            12, 22, 20, 16, 14, 30, 16, 19, 33, 32, 30, 19, 36, 16, 27, 7, 18, 38, 14, 14, 33, 29, 24, 21, 22, 15,\\\n",
    "            25, 27, 25, 37, 35, 34, 11, 19, 35, 10, 30, 8, 11, 20, 7, 27, 19, 16, 21, 13, 6, 29, 35, 13, 31, 23,\\\n",
    "            26, 10, 18, 39, 38, 5, 16, 33, 21, 31, 21, 23, 32, 35, 2, 24, 11, 25, 30, 7, 18, 32, 38, 22, 27, 2, 6,\\\n",
    "            31, 24, 34, 33, 15, 39, 21, 9, 1, 8, 38, 37, 40, 14, 2, 25, 30, 16, 6, 36, 27, 28, 8, 17, 37, 15, 29,\\\n",
    "            27, 30, 30, 19, 15, 13, 34, 5, 24, 18, 40, 37, 1, 28, 17, 32, 8, 34, 5, 6, 31, 8, 9, 28, 26, 40, 40,\\\n",
    "            9, 23, 36, 28, 24, 33, 18, 36, 6, 22, 29, 6, 6, 25, 15, 29, 18, 38, 20, 26, 30, 17, 30, 32, 33, 19,\\\n",
    "            10, 29, 25, 24, 19, 28, 38, 3, 24, 12, 28, 29, 29, 20, 12, 11, 12, 21, 11, 24, 36, 3, 3, 5, 28, 2,\\\n",
    "            8, 30, 23, 4, 40, 28, 6, 31, 37, 25, 9, 23, 20, 20, 16, 38, 21, 35, 18, 3, 15, 40, 19, 33, 34, 20,\\\n",
    "            3, 11, 34, 35, 10, 32, 23, 10, 29, 13, 12, 6, 30, 7, 5, 4, 29, 22, 22, 2, 26, 24, 7, 13, 26, 27, 27,\\\n",
    "            15, 12, 18, 38, 33, 4, 11, 20, 33, 21, 5, 26, 10, 22, 36, 3, 4, 35, 35, 16, 32, 5, 19, 23, 24, 40,\\\n",
    "            25, 30, 10, 9, 23, 12, 40, 21, 29, 18, 17, 15, 32, 2, 35, 7, 30, 4, 2, 16, 6, 8, 35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we can find how many observations we have using the len function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(listBallOne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get to what we are really interested in - whether the number drawn is odd or even.   You'll recall that we can get a 1 to represent an odd number and a 0 to represent an even number with the modulus operator `%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulliBallOneOdd = [x % 2 for x in listBallOne]\n",
    "#print(bernoulliBallOneOdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to start our investigation of the relative number of odd and even numbers that occur in the draws for Ball one by visualising the outcome data in terms of the proportion of odd numbers that are observed in the Ball One draws.  One way to find the number of occurrences of a particular value in a list is to use the list's `count(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulliBallOneOdd.count(1) # find how many 1s there are in the list bernoulliBallOneOdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?list.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that since our Bernoulli random variable outcomes are 1 or 0, we can also count how many odd numbers are drawn by simply adding up the outcomes:  every odd number contributes 1 to the sum and the total is therefore the number of odd numbers drawn.  Doing this over all 1114 observations should give us the same value as counting the number of 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(bernoulliBallOneOdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the equivalent of $t_n = \\displaystyle\\sum_{i=1}^n$, the realisation of the statistic $T_n(X_1, X_2, ..., X_n) = \\sum_{i=1}^n X_i$.\n",
    "\n",
    "We saw that the Bernoulli likelihood function is $L_n(\\theta) = \\theta^{t_n}(1-\\theta)^{(n-t_n)}$ and the log-likelihood function is $l_n(\\theta) = \\log(L_n(\\theta))$ = $t_n$ $\\log(\\theta)$ + $(n-t_n)$ $\\log(1-\\theta)$\n",
    "\n",
    "With our Bernoulli model, our maximum likelihood estimate $\\widehat{\\theta}_n$ for the parameter $\\theta$, the probability that a ball is odd, is $\\frac{t_n}{n}$ which we can see is the same as the proportion of odd-numbered balls in the sample.\n",
    "\n",
    "Using the sum(...) function makes it very easy for us to explore how, as we look at more and more draws (samples), the proportion of odd-numbered balls settles down. \n",
    "\n",
    "Remember the `pylab` function `cumsum` which you can use to calculate the cumulative sum of an array or 'array-like object' (i.e. an object that the function can convert into an array, such as a list or tuple)?  We can use this to give us the cumulative sum of the number of odd-numbered balls in the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "csBernoulliBallOneOdd = np.cumsum(bernoulliBallOneOdd)\n",
    "#print(csBernoulliBallOneOdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do is to visualise the changing proportion of odd numbers as we get more and more samples.  The proportion of odd numbers is the number of odds (calculated as the sum of the outcomes in the samples we have so far) over the total number of samples so far.   To help plotting this we make ourselves a sequence of sample sizes, going up from 1 to 1114:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are at the end of week one of the draws (8 January 1987).  You'll have seen one draw and Ball One was 4, i.e., even.  The proportion of odd numbers is from this one draw is 0 ( = 0/1).  Then week two comes along, there is another draw, the number is 3 (odd) and so the proportion of odds in the 2 samples so far is 1/2.  Then week 3 (11, odd), so the proportion is 2/3, etc etc etc.   After each draw, we are dividing the cumulative sum of the outcomes by the number of draws to date.    If we kept doing this week after week we'd get something like this, ending up with 546 odds out of 1114 observations which simplifies to 273/557."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relFreqs = csBernoulliBallOneOdd/np.arange(1,1115)\n",
    "relFreqsCsBernoulliBallOneOdd = np.stack([np.arange(1,1115),relFreqs],axis=-1)\n",
    "#print(relFreqsCsBernoulliBallOneOdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a slight variation on our list comprehension, we can make a list of points out of this, plotting the proportion of odds on the y-axis against the number of observations on the x-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(relFreqsCsBernoulliBallOneOdd[:,0],relFreqsCsBernoulliBallOneOdd[:,1],s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have effectively plotted the maximum likelihood estimate or MLE $\\widehat{\\theta}_n$ for $\\theta$ in our $Bernoulli$ model over increasing values of $n$.\n",
    "\n",
    "We can also look at the shape of the whole log-likelihood function, not just the value that maximises it.\n",
    "\n",
    "This interactive plot draws the log-likelihood function for samples based on for different values of $n$.  Again, for $n=1$, you have only observed the first draw, for $n=2$ you have observed two draws, etc etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def _(n=IntSlider(1,1,1114,100)):\n",
    "    '''Interactive function to plot sample-size specific log likelihood function.'''\n",
    "    if n == 1:\n",
    "        print( \"Log-likelihood function based on first sample\")\n",
    "    else:\n",
    "        print( \"log-likelihood function based on\", n,  \"samples\"    )\n",
    "    tn = csBernoulliBallOneOdd[n-1]\n",
    "    #theta = var('theta')\n",
    "    thetas = np.linspace(0,1,100)\n",
    "    plt.plot(thetas,(1/n)*(tn*np.log(thetas)+(n-tn)*np.log(1-thetas)))\n",
    "    #show(plot((tn * log(theta) + (n - tn) * log(1-theta)), theta,0,1),figsize=[8,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing $n$ to see how the shape of the log-likelihood function changes as we get more and more observations.  \n",
    "\n",
    "We can also show the log likelihood functions for a number of different value of $n$ all on the same plot.  The first cell below shows log-likelihood functions for $n = 1$ to $n = 20$.  This is where the log-likelihood moves around most as $n$ changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(1,20)\n",
    "1-((20-n)/20)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1,20,1):\n",
    "    tn = csBernoulliBallOneOdd[n-1]\n",
    "    thetas = np.linspace(0,1,100)\n",
    "    plt.plot(thetas,(tn*np.log(thetas)+(n-tn)*np.log(1-thetas)),color='blue',linewidth=0.2)\n",
    "    #p += plot((Tn * log(theta) + (n-Tn)*log(1-theta)), theta,0,1)\n",
    "#show(p, figsize=[6,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sympy for Basic Algebra and Calculus\n",
    "\n",
    "When we wanted to differentiate the log-likelihood $l_n(\\theta)$ above, we did it for ourselves, but Sympy could have helped us to do even that. \n",
    "\n",
    "Sage can be used to find solutions to equations and for basic calculus.  The secret is to create a symbolic expression using the `var(...)` function.  It is probably easiest to think of var as a way to tell Sympy that something is a variable name without having to assign that variable to an actual value. Then, using the function  solve  we can solve equations, i.e. use Sympy to find the value(s) of a variable which would solve the equation of interest, or expressions for one variable in terms of other variables involved in the equation(s).\n",
    "\n",
    "The examples used here are taken from the book Sage Tutorial, The Sage Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import var\n",
    "x = var('x') # symbolic expression\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncommnet next line to read the docs of `var` as needed\n",
    "#?var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncommnet next line to read the docs of `solve` as needed\n",
    "#?solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a simple example, solving $x^2 + 3x + 2 = 0$ for $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import solve\n",
    "solve(x**2 + 3*x + 2, x,dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `solve` if we have variables instead of known values for the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Symbol\n",
    "x, a, b, c = var('x a b c')\n",
    "solve(a*(x**2) + b*x + c, x,dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can solve a system of equations for several variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = var('x y')\n",
    "solve([x+y-6, x-y-4], x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes Sympy cannot find an exact solution to the equation, as in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import cos,exp\n",
    "theta = var('theta')\n",
    "solve([cos(theta)-exp(theta)], theta,dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can try using the `nsolve` function to find a numerical solution.  Note that as well as the equation, you have to pass `nsolve(...)` the first guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import nsolve\n",
    "nsolve(cos(theta)-exp(theta), theta,-1,dict=True,set=(-1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncommnet next line to rwead the docs of `find_root` as needed\n",
    "#?nsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use calculus with our symbolic expressions.  We differentiate with the diff function or method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import diff,sin\n",
    "u = var('u')\n",
    "diff(sin(u), u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also tackle higher derivatives, such as the fourth derivative of $\\sin(x^2)$ in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = var('x')\n",
    "diff(sin(x**2), x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncommnet next line to rwead the docs of `diff` as needed\n",
    "#?diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial derivatives can also be found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = var('x y')\n",
    "f = x**2 + 17*y**2\n",
    "diff(f,x) # differentiate f with respect to x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = var('x y')\n",
    "f = x**2 + 17*y**2\n",
    "diff(f,y) # differentiate f with respect to y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `integrate(...)` function does integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import integrate\n",
    "x = var('x')\n",
    "integrate(x*sin(x**2), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about integrals that dont have closed form solutions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import E\n",
    "x = var('x')\n",
    "integrate(E**(x**2),x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic Expressions for the Maximum Likelihood Estimator\n",
    "\n",
    "We can use these SageMath capabilities to help us to find maximum likelihood estimators.   We will first have to find an expression for the likelihood of the parameter(s) in terms of of some statistic or statistics of the observations.  We then take logs to get a log-likelihood function (since logs are usually easier to work with).  Then, with the Sage diff function and the solve function, we have some powerful tools to then help us to differentiate and find the value at which the differential is 0. \n",
    "\n",
    "Let's start with the Bernoulli log-likelihood function $l_n(\\theta) = \\log( L_n(\\theta))$ = $t_n$ $\\log(\\theta)$ + $(n-t_n)$ $\\log(1-\\theta)$ and first of all find an expression for the differential of this with respect to $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import log\n",
    "theta, n, tn = var('theta n tn')\n",
    "logL = tn*log(theta) + (n-tn)*log(1-theta) # Bernoulli log likelihood\n",
    "dlogL = diff(logL,theta)\n",
    "dlogL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then solve for $\\theta$ when the differential is zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(dlogL, theta,dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magic!  We get the expression for $\\widehat{\\theta}_n$ that we derived before!\n",
    "\n",
    "#### YouTry later\n",
    "\n",
    "Try `diff`, `integral`, and `solve` on some other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "lx_course_instance": "2022",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
