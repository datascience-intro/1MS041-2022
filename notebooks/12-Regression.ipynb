{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science](http://datascience-intro.github.io/1MS041-2022/)    \n",
    "## 1MS041, 2022 \n",
    "&copy;2022 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by talking about a few examples of supervised learning problems. Suppose we have a dataset giving the living areas and prices of 47 houses from Portland, Oregon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/portland.csv')\n",
    "print(len(df))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets say that our goal would be to predict the price of the house given the size and the number of bedrooms\n",
    "\n",
    "In the case of simple linear regression we could set $x$ to be the size in square feet and $y$ to be the price, the goal would then be to find a function $f(x)$ that is close to $y$ in some sense.\n",
    "\n",
    "In the context of machine learning they often use the following terminology: let $x^{(i)}$ denote the **features**(living area) and let $y^{(i)}$ denote the **target** (price), then a pair $(x^{(i)},y^{(i)}$ would be called a **training example**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this terminology they also call the set of observations $\\{(x^{(i)},y^{(i)}),\\, i=1,\\ldots,m\\}$ a training set. \n",
    "\n",
    "> **In this context the goal is statistical prediction**\n",
    "\n",
    "> Contrast this with the **statistical estimation** viewpoint of linear regression, where the goal is to estimate the parameters.\n",
    "\n",
    "Why is this difference, basically it is one of explainability. Estimation is often used as a tool to explain something through its statistical model and the estimated parameters of the model. Lets assume that there is a linear relationship between fat percentage and BMI, but we do not know the parameters. Then by simply taking a few observations and performing a parameter estimation under a given loss, such as the maximum likelihood estimator (MLE), we can do hypothesis tests to check if the parameters are positive or test between different proposed values of said parameters. The goal in statistical machine learning is often one of prediction, and as you will see, the models that are often in use, do not allow us to actually explain anything, although the prediction is also accomplished by first estimating parameters of a model but with the explicit goal of predicting future from past observations.\n",
    "\n",
    "> In conclusion, in statistical machine learning we are often using weaker model assumptions, but since we are focusing on prediction we do not really have a problem. In contrast, in classical statistical decision problems, the focus is on stronger model assumptions and the goal is to extract more detailed information about the relationships between features and targets to obtain a better explainable understanding of the underlying data generating process.\n",
    "\n",
    "> Think of the name, machine learning. From this you get that the focus has to be the behavior of the machine (prediction).\n",
    "\n",
    "It is important to bear in mind that estimation for explainability and estimation for predictability are both formally statistical decision problems. Here, we take such a mathematical approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Portland house price example using Sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import showURL\n",
    "showURL('https://scikit-learn.org/stable/',600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use sci-kit learns framework to \"train\" a linear regression model we will first have to prepare the data in the way that it expects. The format is as follows\n",
    "\n",
    "* X -- a numpy array of shape (n_samples,n_features)\n",
    "* Y -- a numpy array of length n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[['Size of the house (in square feet)','Number of bedrooms']].to_numpy() # To convert from dataframe to numpy array\n",
    "Y = df['Price of the house'].to_numpy()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state=0,test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's note the shapes of `X` and `Y` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape,X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now gives us a fitted model for this particular data, so lets plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_train[:,0],Y_train)\n",
    "plt.scatter(X_train[:,0],lr.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_test[:,0],Y_test)\n",
    "plt.scatter(X_test[:,0],lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, since the x-axis is size and the y axis is price we have an underlying variable which is number of bedrooms the line is not straight.\n",
    "\n",
    "Let's also plot the number of bedrooms `x[1]` against the price (y-axis) next to appreciate the other discrete feature.\n",
    "\n",
    "But remember, this is a linear model so if we consider the full 3-d space the predictions would be on a plane instead of a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:,1],Y_train)\n",
    "plt.scatter(X_train[:,1],lr.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:,1],Y_test)\n",
    "plt.scatter(X_test[:,1],lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_train,Y_train) # Score returns R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test,Y_test) # Score returns R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "Often we want to know if our predictions are calibrated, the concept is easiest to understand simply by looking at the following plot of the predicted value versus the true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lr.predict(X_train),Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it looks quite good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lr.predict(X_test),Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, maybe not so good. We are underestimating the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here calibration would mean that the predictions and the true values \"follow\" a straight line with slope $1$ and intercept $0$. Actually the mean square calibration error is the following\n",
    "$$\n",
    "    \\mathbb{E}[|\\mathbb{E}[Y \\mid f(X)] - f(X)|^2]^{1/2}\n",
    "$$\n",
    "Thus it is checking for every predicted value $f(X)$ the variance of the true values, we want this to be small.\n",
    "Lets compute the calibration error here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(X_test).reshape(-1,1)\n",
    "\n",
    "lr_calib = LinearRegression()\n",
    "lr_calib.fit(predictions,Y_test)\n",
    "\n",
    "calibration_residual = (lr_calib.predict(predictions)-predictions)\n",
    "np.sqrt(np.mean(calibration_residual**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((Y_test-predictions)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predictions,lr_calib.predict(predictions))\n",
    "plt.scatter(predictions,Y_test)\n",
    "plt.scatter(predictions,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we can see that we are not entirely calibrated, but that we have a simple additive bias, i.e. we are underestimating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try another model and see if we can change the calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression()\n",
    "lr2.fit(X_train[:,0:1]**2,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lr2.predict(X_test[:,0:1]**2),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr2 = lr2.predict(X_test[:,0:1]**2).reshape(-1,1)\n",
    "\n",
    "lr2_calib = LinearRegression()\n",
    "lr2_calib.fit(predictions_lr2,Y_test)\n",
    "\n",
    "calibration_residual_lr2 = (lr2_calib.predict(predictions_lr2)-predictions_lr2)\n",
    "np.sqrt(np.mean(calibration_residual_lr2**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predictions_lr2,lr2_calib.predict(predictions_lr2))\n",
    "plt.scatter(predictions_lr2,Y_test)\n",
    "plt.scatter(predictions_lr2,predictions_lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((Y_test-predictions_lr2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a bit more complicated, we are always underestimating, but less so at smaller prices and more at higher prices. Interestingly this gives a smaller calibration error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calibration error is a bit tricky to get confidence bounds for, but can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about measuring the model\n",
    "\n",
    "We will illustrate this with an example built upon the Portland data that we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import numpy as np\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "california_housing = datasets.fetch_california_housing()\n",
    "print(california_housing['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = california_housing.data\n",
    "Y = california_housing.target\n",
    "# For the purpose of exposition we normalize the Y variable between 0 and 1\n",
    "Y = Y/np.max(Y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,train_size=0.9,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_train,Y_train` will now be different from `X_test,Y_test`. What this means is that if we assume that the original data is IID we can consider the two samples independent. So, let us train a simple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = Y_test-lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import plotEDF, makeEDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf = makeEDF(residual)\n",
    "plotEDF(edf,points_at_jump=False,confidence_band=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(X_test).reshape(-1,1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "lr_calib = RandomForestRegressor(min_samples_leaf=10)\n",
    "lr_calib.fit(predictions,Y_test)\n",
    "\n",
    "calibration_residual = (lr_calib.predict(predictions)-predictions)\n",
    "np.sqrt(np.mean(calibration_residual**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(predictions,Y_test,alpha=0.1)\n",
    "plt.scatter(predictions,lr_calib.predict(predictions),alpha=0.1)\n",
    "plt.scatter(predictions,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the mean square error?\n",
    "$$\n",
    "    \\mathbb{E}[|Y-f(X)|^2]^{1/2} = \\left ( \\mathbb{E}[|\\mathbb{E}[Y \\mid f(X)]-f(X)|^2+|Y-\\mathbb{E}[Y \\mid f(X)]|^2] \\right )^{1/2}\n",
    "$$\n",
    "\n",
    "The first term is the calibration term (can be thought of as bias) and the second term is just the variance at the predicted value. For this particular problem, most of the contribution comes from the calibration error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((Y_test-predictions)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring how good a model is (explained variance)\n",
    "\n",
    "The **coefficient of determination** or **explained variance** is defined as follows:\n",
    "\n",
    "$$R^2 = 1- \\frac{MSE}{Var(y)}$$\n",
    "\n",
    "MSE - Mean Squared Error and is the sum of squares of the residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it fit the exposition we did in the chapter about regression and finding confidence bounds for $R^2$ or FVU explicitly we need to figure out the max and min of the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(Y_test)\n",
    "alpha=0.05\n",
    "import scipy.optimize as so\n",
    "h = lambda u: (1+u)*np.log(1+u)-u\n",
    "sigma2 = np.var(np.power(Y_test-np.mean(Y_test),2))\n",
    "f = lambda epsilon: np.exp(-n*sigma2*h(epsilon/sigma2))-alpha/2\n",
    "ans = so.fsolve(f,0.001)\n",
    "epsilon2 = np.abs(ans[0])\n",
    "print(epsilon2,f(epsilon2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.power(residual,2)\n",
    "sigma2 = np.var(X)\n",
    "b = np.max(X)-np.min(X)\n",
    "f = lambda epsilon: np.exp(-n*sigma2/(b**2)*h(b*epsilon/sigma2))-alpha/2\n",
    "ans = so.fsolve(f,0.001)\n",
    "epsilon1 = np.abs(ans[0])\n",
    "print(epsilon1,f(epsilon1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerBound = (np.mean(np.power(residual,2))-epsilon1)/(np.var(Y_test,ddof=1)+epsilon2)\n",
    "upperBound = (np.mean(np.power(residual,2))+epsilon1)/(np.var(Y_test,ddof=1)-epsilon2)\n",
    "print(lowerBound,upperBound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us the confidence interval for the FVU using Bennett's inequality. This is not too bad, and we could get smaller by having a bigger test set.\n",
    "\n",
    "Key takeaway here is that, for measuring regression performance we often need more testing data than we need for the classification problems. This is essentially due to the metrics being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More interesting example\n",
    "\n",
    "In our derivation, we might as well have considered multiple features, like multiple linear regression. The extension is the same, now $\\beta_0$ is still a number, but $\\beta_1,x$ are vectors in $\\mathbb{R}^d$ where $d$ is the number of features, $f(x) = \\beta_0 + \\beta_1 \\cdot x$. With this simple extension we can consider a more interesting example. Consider a dataset of 8x8 bitmaps representing handwritten digits, this can look like follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "fig, ax = plt.subplots(2,5)\n",
    "plt.gray()\n",
    "for i in range(10):\n",
    "    from math import floor\n",
    "    row = floor(i/5)\n",
    "    column = i % 5\n",
    "    ax[row,column].imshow(digits['data'][i,:].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first build a classifier that distinguishes the  top row from the bottom row, so let us construct the target for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = (digits['target'] >= 5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(digits['data'],target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(sc.transform(X_train),Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg.score(sc.transform(X_train),Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can with the same methods as before construct confidence bands around the residual ECDF using the DKW inequality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import makeEDF,plotEDF\n",
    "edf = makeEDF(logReg.predict(X_test)-Y_test)\n",
    "plotEDF(edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(logReg.predict_proba(X_test)[:,1],Y_test,alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logReg.predict_proba(X_test)[:,1].reshape(-1,1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "lr_calib = RandomForestRegressor(min_samples_leaf=30)\n",
    "lr_calib.fit(predictions,Y_test)\n",
    "\n",
    "calibration_residual = (lr_calib.predict(predictions)-predictions)\n",
    "np.sqrt(np.mean(calibration_residual**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(predictions,Y_test,alpha=0.1)\n",
    "plt.scatter(predictions,lr_calib.predict(predictions),alpha=0.1)\n",
    "plt.scatter(predictions,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you see its not very well calibrated. Simply because we are not minimizing the $L^2$ norm, we are minimizing the cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((Y_test-predictions)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see that the root mean square error (RMS) is roughly of the same size as the calibration error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple classes\n",
    "\n",
    "The above example naturally leads us to wanting to model multiple outputs. That is, instead of the Bernoulli we could consider DeMoivre$(p_1,\\ldots,p_m)$ for $m$ classes. What we want is the following\n",
    "\n",
    "$$\n",
    "    \\sum_{i=1}^m p_i = 1\n",
    "$$\n",
    "\n",
    "$Y_i \\mid X_i \\sim \\text{DeMoivre}(\\theta(X_i))$, where $\\theta \\in [0,1]^m$. But how do we find a good model for $\\theta$?\n",
    "\n",
    "Let us model each log-ratio as a linear function\n",
    "$$\n",
    "    \\log\\left ( \\frac{P(Y = i \\mid X)}{P(Y = m \\mid X)}\\right ) = w_{i} \\cdot x, \\quad \\forall i=1,\\ldots,m-1\n",
    "$$\n",
    "now fix $i$ and consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    P(Y = i \\mid X) = e^{w_i \\cdot x} P(Y = m \\mid X), \\quad \\forall i=1,\\ldots, m-1\n",
    "$$\n",
    "Now\n",
    "$$\n",
    "    \\sum P(Y = i \\mid X) = 1\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "    P(Y = m \\mid X) = 1-\\sum_{i=1}^{m-1} P(Y = i \\mid X) = 1-\\sum_{i=1}^{m-1} e^{w_i \\cdot x} P(Y = m \\mid X)\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "    P(Y = m \\mid X) = \\frac{1}{1+\\sum_{i=1}^{m-1} e^{w_i \\cdot x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging back in gives\n",
    "$$\n",
    "    P(Y = i \\mid X) = \\frac{e^{w_i \\cdot x}}{1+\\sum_{j=1}^{m-1} e^{w_j \\cdot k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(digits['data'],digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(sc.transform(X_train),Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg.score(sc.transform(X_train),Y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "lx_course_instance": "2022",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
